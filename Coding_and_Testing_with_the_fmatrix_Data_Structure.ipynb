{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coding and Testing with the fmatrix Data Structure.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iwd38XvZK2sy",
        "0fsEMpauK8lW",
        "N45Tn8noN12W",
        "HnGphOO21Sc7",
        "5M-PpUH6XFdg",
        "zDT5XgNRjshL",
        "AwXyYEzjx-o2",
        "mHzL7q1TOPOD",
        "qrqmxMH0c4E_",
        "_iBCyYjIlvwp",
        "_Vn-fPdVz9RC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwd38XvZK2sy"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm8KscV-wx3m"
      },
      "source": [
        "Check that the NVIDIA compiler is present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ewR7_sHJ00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77814391-0e29-4373-aba5-9ff0f117adee"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TEvrWMJ_a"
      },
      "source": [
        "## CUDA Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-lgwhE1N5_7",
        "outputId": "56c83c1a-9353-417e-c066-48e468b3adac"
      },
      "source": [
        "%%writefile cuda_stuff.cuh\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#ifndef cuda_stuff_H\n",
        "#define cuda_stuff_H\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   nb_rows = number of rows */\n",
        "#define IDX2C(i,j,nb_rows) (((j)*(nb_rows))+(i))\n",
        " \n",
        "//MACRO TO DEBUGG CUDA FUNCTIONS\n",
        "/** Error checking,\n",
        " *  taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess) \n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "/** Error checking for use with CUDA Dynamic Parallelism */\n",
        "/*\n",
        "#define cdpErrchk(ans) { cdpAssert((ans), __FILE__, __LINE__); }\n",
        "__device__ void cdpAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      printf(\"GPU kernel assert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) assert(0);\n",
        "   }\n",
        "}\n",
        "*/\n",
        "void device_synchronize();\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda_stuff.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivrxLaYOYPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da91cc6f-ba4e-4019-aa94-b94d24092de5"
      },
      "source": [
        "%%writefile cuda_stuff.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "\n",
        "void device_synchronize(){\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "}"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda_stuff.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsEMpauK8lW"
      },
      "source": [
        "# The fmatrix Data Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD7TXq-1L8tL"
      },
      "source": [
        "Similar to cuBLAS, a 2D matrix is represented by a 1D array that stores one column of the matrix after another (this is called _linearization_).\n",
        "The `fmatrix` structure contains a `data` pointer to such an array, which is 100% compatible with cuBLAS. In addition, it stores the number of columns and rows in the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A97U902HMog4",
        "outputId": "12f59b7e-1f16-40c1-82ca-1794d1cb8dae"
      },
      "source": [
        "%%writefile fmatrix.cuh\n",
        "#ifndef fmatrices_H\n",
        "#define fmatrices_H\n",
        "#include \"cuda_stuff.cuh\" // for IDX2C\n",
        "\n",
        "////////////////////////////////////////\n",
        "// basic data structure and access macro\n",
        "////////////////////////////////////////\n",
        "typedef struct {\n",
        "    float* data;\n",
        "    int cols;\n",
        "    int rows;\n",
        "} fmatrix;\n",
        "\n",
        "/** Access element (i,j) of matrix M \n",
        " *\n",
        " *  Usage example:\n",
        " *  For computing A = B^T + C), loop over i and j with:\n",
        " *    getfm(A,i,j) = getfm(B,j,i) + getfm(C,i,j);\n",
        " **/\n",
        "#define getfm(M,i,j) (M.data[IDX2C(i,j,M.rows)])\n",
        "\n",
        "////////////////////////////////////////\n",
        "// utility functions\n",
        "////////////////////////////////////////\n",
        "/** Returns the number of elements in the matrix.\n",
        " *\n",
        " *  Useful for computing, e.g., the size\n",
        " *  of a 1D-vector that contains the same numbers.\n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "int fmatrix_elements(fmatrix mat);\n",
        "\n",
        "/** Returns the memory occupied by the matrix elements in bytes\n",
        " *  (not including the variables in the struct mat).\n",
        " *\n",
        " *  Useful for allocating memory for the data.\n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "int fmatrix_size(fmatrix mat);\n",
        "\n",
        "/** Assert that the matrix is coherent: all fields nonzero. */\n",
        " __host__\n",
        " __device__\n",
        "void fmatrix_assert(fmatrix mat);\n",
        "\n",
        "////////////////////////////////////////\n",
        "// Create, copy, destroy\n",
        "////////////////////////////////////////\n",
        "/** Allocate memory on host */\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols);\n",
        "\n",
        "/** Allocate memory on device */\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols);\n",
        "\n",
        "/** Create a matrix representing columns [a,b) of M. \n",
        " *  Note that the new matrix uses a pointer to the\n",
        " *  data of M. The data is not copied to a new location.\n",
        " *  If M is destroyed, this matrix is useless.\n",
        " */\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b);\n",
        "\n",
        "/** Copy data from matrix on device to host \n",
        " *  (no memory allocation). */\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device);\n",
        "\n",
        "/** Copy data from matrix on host to device\n",
        " *  (no memory allocation). */\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device);\n",
        "\n",
        "/** Copy matrix from device to host, allocating new memory. */\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_device);\n",
        "\n",
        "/** Copy matrix from host to device, allocating new memory. */\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host);\n",
        "\n",
        "/** Free data memory on host. \n",
        " *  This zeros out the data pointer of the fmatrix struct,\n",
        " *  so a pointer is required. */\n",
        "void fmatrix_free_on_host(fmatrix* mat);\n",
        "\n",
        "/** Free data memory on device. \n",
        " *  This zeros out the data pointer of the fmatrix struct,\n",
        " *  so a pointer is required. */\n",
        "void fmatrix_free_on_device(fmatrix* mat);\n",
        "\n",
        "////////////////////////////////////////\n",
        "// Input and Output\n",
        "////////////////////////////////////////\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the host. \n",
        " *  If nb<0, print all rows. \n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "void fmatrix_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the device. \n",
        " *  If nb<0, print all rows. \n",
        " *\n",
        " *  This version copies the matrix to host first.\n",
        " */\n",
        "void fmatrix_device_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print a matrix to a csv file. \n",
        " *\n",
        " *  This version copies the matrix to host first.\n",
        " */\n",
        "void fmatrix_device_to_csv(const char* filename, fmatrix mat);\n",
        "\n",
        "/** Read a matrix from a csv file. \n",
        " *\n",
        " *  This version creates the matrix on the host first.\n",
        " */\n",
        "fmatrix fmatrix_device_from_csv(const char* filename);\n",
        "\n",
        "////////////////////////////////////////\n",
        "// Useful\n",
        "////////////////////////////////////////\n",
        "\n",
        "/** Create a matrix with random values between -1 and 1\n",
        " *  on the device */\n",
        "fmatrix fmatrix_create_random_on_device(int rows, int cols);\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGwZ36ifWQ-d",
        "outputId": "5bdd9867-e0a7-4541-d823-c9c82a0f9ddd"
      },
      "source": [
        "%%writefile fmatrix.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include <curand.h>\n",
        "#include <curand_kernel.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "// for reading CSV files, we use some C++\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <fstream>\n",
        "#include <string>\n",
        "\n",
        "int fmatrix_elements(fmatrix mat) {\n",
        "     return mat.cols*mat.rows;\n",
        "}\n",
        "\n",
        "int fmatrix_size(fmatrix mat) {\n",
        "    fmatrix_assert(mat);\n",
        "     return fmatrix_elements(mat) * sizeof(mat.data[0]);\n",
        "}\n",
        "\n",
        "void fmatrix_assert(fmatrix mat) {\n",
        "    assert(mat.data);\n",
        "    assert(mat.cols);\n",
        "    assert(mat.rows);\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    mat.data = (float*)malloc(fmatrix_size(mat)); \n",
        "    assert(mat.data);\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    gpuErrchk( \n",
        "        cudaMalloc((void **)&(mat.data), fmatrix_size(mat)) \n",
        "    );\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk( \n",
        "        cudaMemcpy( mat_device.data, mat_host.data, \n",
        "                   fmatrix_size(mat_host), \n",
        "                   cudaMemcpyHostToDevice \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_host.data, mat_device.data,  \n",
        "                   fmatrix_size(mat_device), \n",
        "                   cudaMemcpyDeviceToHost \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_device);\n",
        "    fmatrix mat_host = fmatrix_create_on_host(mat_device.rows, mat_device.cols);\n",
        "    fmatrix_data_to_host(mat_host,mat_device);\n",
        "    return mat_host;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix mat_device = fmatrix_create_on_device(mat_host.rows, mat_host.cols);\n",
        "    fmatrix_data_to_device(mat_host,mat_device);\n",
        "    return mat_device;\n",
        "}\n",
        "\n",
        "/** We could do it like this, but it would not set our pointer M.data to 0. \n",
        "... fmatrix_free_on_host(M)\n",
        "void fmatrix_free_on_host(fmatrix mat) {\n",
        "    fmatrix_assert(mat);  \n",
        "  free(mat.data);\n",
        "  mat.data = 0;\n",
        "  mat.cols = 0;\n",
        "  mat.rows = 0;\n",
        "}\n",
        "*/\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  free(mat->data);\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_device(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  gpuErrchk(cudaFree(mat->data));\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b) {\n",
        "    fmatrix_assert(M);  \n",
        "    fmatrix A = { \n",
        "        .data = &getfm(M,0,a),  \n",
        "        .cols = b-a,\n",
        "        .rows = M.rows \n",
        "    };\n",
        "    fmatrix_assert(A);  \n",
        "    return A;\n",
        "}\n",
        "\n",
        "\n",
        "__host__\n",
        "__device__\n",
        "void fmatrix_print(fmatrix mat, int nb){\n",
        "    if (nb<0 || nb > mat.rows) {\n",
        "        nb = mat.rows;\n",
        "    }\n",
        "    printf(\"[\\n\");\n",
        "    for (int i = 0 ; i < nb; i++){\n",
        "      for (int j = 0 ; j<mat.cols; j++){\n",
        "        printf(\"%f\", getfm(mat,i,j));\n",
        "        if (j+1<mat.cols) {\n",
        "          printf(\",\\t\");\n",
        "        }\n",
        "      }\n",
        "      if (i+1<nb) {\n",
        "        printf(\";\\n\");\n",
        "      }\n",
        "    }\n",
        "    if (nb < mat.rows) {\n",
        "      printf(\"\\n...\\n\");\n",
        "    }\n",
        "  printf(\"\\n]\\n\");\n",
        "}\n",
        "\n",
        "void fmatrix_device_print(fmatrix mat, int nb){\n",
        "   // allocate copy\n",
        "   fmatrix tmp = fmatrix_copy_to_host(mat);\n",
        "   fmatrix_print(tmp,nb);\n",
        "   fmatrix_free_on_host(&tmp);\n",
        "}\n",
        "\n",
        "void fmatrix_device_to_csv(const char* filename, fmatrix mat) {\n",
        "  // Open file\n",
        "  FILE* fp = fopen(filename, \"w\");\n",
        "  // allocate copy\n",
        "  fmatrix tmp = fmatrix_copy_to_host(mat);\n",
        "  for (int i = 0 ; i < tmp.rows; i++){\n",
        "    for (int j = 0 ; j<tmp.cols; j++){\n",
        "      // Note: %.15g gives 15 significant digits (full double precision)\n",
        "      fprintf(fp,\"%.15g\", getfm(tmp,i,j));\n",
        "      if (j+1<tmp.cols) {\n",
        "        fprintf(fp,\",\");\n",
        "      }\n",
        "    }\n",
        "    fprintf(fp,\"\\n\");\n",
        "  }\n",
        "  fmatrix_free_on_host(&tmp);\n",
        "  // Close file\n",
        "  fclose(fp);\n",
        "}\n",
        "\n",
        "__global__\n",
        "void fmatrix_create_random_on_device_kernel(fmatrix M) {\n",
        "    // choose a seed (here: the same each launch)\n",
        "    unsigned long seed = 0;\n",
        "    int sequence = 0;\n",
        "    // first, initialize the random numbers\n",
        "    curandState state;\n",
        "    curand_init(seed, sequence, 0, &state);\n",
        "    for (int i = 0; i < fmatrix_elements(M); ++i) {\n",
        "        // curand_uniform creates numbers between 0 and 1\n",
        "        M.data[i] = (curand_uniform(&state)-0.5)*2.0;\n",
        "    }\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_random_on_device(int rows, int cols) {\n",
        "    // Create an uninitialized matrix on the device\n",
        "    fmatrix M = fmatrix_create_on_device(rows,cols);\n",
        "    // Call a kernel with a single thread to fill the values\n",
        "    fmatrix_create_random_on_device_kernel<<<1,1>>>(M);\n",
        "\n",
        "    return M;\n",
        "}\n",
        "\n",
        "/* Count the number of rows and columns in a csv files (without headers) */\n",
        "void count_elements_in_csv(const char* filename, int* rows, int* cols) {\n",
        "  // Note: for the sake of convenience, we use some C++ functions here\n",
        "  using namespace std;\n",
        "\n",
        "  *rows = 0;\n",
        "  *cols = 0;\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "    while (getline(infile, row_as_string, '\\n')) {\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "        int tempcols = 0;\n",
        "        while (getline(line_stream, value, ',')) {\n",
        "          ++tempcols;\n",
        "        }\n",
        "        if (tempcols > *cols) {\n",
        "           *cols = tempcols;\n",
        "        }\n",
        "        ++(*rows);\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n",
        "/** Read the data from a csv file into an fmatrix on the host.\n",
        " *  Careful: We assume that the matrix has the right dimensions!\n",
        " *  Use count_elements_in_csv(...) to get the dimensions if\n",
        " *  unknown.\n",
        " */\n",
        "void fmatrix_fill_from_csv(fmatrix h_M,const char* filename) {\n",
        "  // Note: for the sake of convenience, we use some C++ functions here\n",
        "  using namespace std;\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "  int row = 0;\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "    while (getline(infile, row_as_string, '\\n')) {\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "        int col = 0;\n",
        "        while (getline(line_stream, value, ',')) {\n",
        "\t\t\t\t\tgetfm(h_M,row,col) = strtod(value.c_str(), NULL); \n",
        "          ++col;\n",
        "\t\t\t\t}\n",
        "        ++row;\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_device_from_csv(const char* filename) {\n",
        "  // first read the file to count the number of elements\n",
        "  int rows = 0;\n",
        "  int cols = 0;\n",
        "  count_elements_in_csv(filename,&rows,&cols);\n",
        "\n",
        "  // allocate the matrix on the host\n",
        "  fmatrix h_M = fmatrix_create_on_host(rows,cols);\n",
        "\n",
        "  // read the data into the host matrix\n",
        "  fmatrix_fill_from_csv(h_M,filename);\n",
        "\n",
        "  // copy the matrix to the device\n",
        "  fmatrix M = fmatrix_copy_to_device(h_M);\n",
        "  \n",
        "  // destroy the host matrix\n",
        "  fmatrix_free_on_host(&h_M);\n",
        "\n",
        "  return M;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN57hYCW_Qug"
      },
      "source": [
        "# Writing Matrix Functions with fmatrix\n",
        "We will look at two examples that illustrate how to create, manipulate and compute using the fmatrix structure.\n",
        "The first example is the matrix transpose, which generates a new matrix.\n",
        "The second example is the matrix multiplication. It overwrites the data of an existing matrix with the result of the multiplication.\n",
        "\n",
        "We start by looking at the code. Testing the code will be discussed in the section that comes afterwards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N45Tn8noN12W"
      },
      "source": [
        "## Matrix Addition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFrZCF7jOse1",
        "outputId": "ecde9878-6330-4f40-ca36-040a5798c277"
      },
      "source": [
        "%%writefile fmatrix_add.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "__global__ \n",
        "void fmatrix_add_kernel(fmatrix P,float a,fmatrix Y) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        getfm(P,i,j) += a*getfm(Y,i,j);\n",
        "    }\n",
        "}\n",
        "\n",
        "/** Compute P = P + a*Y */\n",
        "void fmatrix_add(fmatrix P,float a,fmatrix Y) {\n",
        "    fmatrix_assert(P);\n",
        "    fmatrix_assert(Y);\n",
        "    assert(P.rows == Y.rows);\n",
        "    assert(P.cols == Y.cols);\n",
        "    int threadsPerBlock = fmatrix_elements(P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_add_kernel<<< blocksPerGrid, threadsPerBlock >>>(P,a,Y);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the implementation for add doesn't include any synchronization. This is so that you can launch adds in parallel, which is perfectly fine if the date for both tasks is independent. Here's a small example:\n",
        "```\n",
        "// Without waiting:\n",
        "fmatrix_add(P1,a,Y1);\n",
        "fmatrix_add(P2,a,Y2);\n",
        "deviceSynchronize();\n",
        "\n",
        "// Waiting between each\n",
        "fmatrix_add(P1,a,Y1);\n",
        "deviceSynchronize();\n",
        "fmatrix_add(P2,a,Y2);\n",
        "deviceSynchronize();\n",
        "```"
      ],
      "metadata": {
        "id": "X0SLSDrV3CLJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnGphOO21Sc7"
      },
      "source": [
        "## Matrix Multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNY5FKdl19Bc"
      },
      "source": [
        "Here, we will create a new matrix A that contains the product of two matrices B and C.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g1XwwS419Bn",
        "outputId": "1f3a3c9b-bd4c-47c3-a385-f6d9bee54108"
      },
      "source": [
        "%%writefile fmatrix_mult.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__\n",
        "void fmatrix_multiplication_kernel(fmatrix A, float f, fmatrix B, fmatrix C) {\n",
        "    // Each thread multiplies one row of B with one column of C\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / A.rows;\n",
        "    int i = idx % A.rows;\n",
        "    if (i < A.rows && j < A.cols ){\n",
        "        getfm(A,i,j) = 0.0;\n",
        "        for (int k = 0; k < B.cols; ++k) {\n",
        "          getfm(A,i,j) += f*getfm(B,i,k)*getfm(C,k,j);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Compute A = f*B*C */\n",
        "void fmatrix_mult(fmatrix A, float f, fmatrix B, fmatrix C) {\n",
        "    // First let's check for errors in the argument M.\n",
        "    // This can help a LOT when debugging.\n",
        "    // A,B,C need to have nonzero pointers etc.\n",
        "    // fmatrix_assert(A);\n",
        "    // fmatrix_assert(B);\n",
        "    // fmatrix_assert(C);\n",
        "    assert(A.rows == B.rows);\n",
        "    assert(A.cols == C.cols);\n",
        "    assert(B.cols == C.rows);\n",
        "    \n",
        "    // take one thread per element, and distribute\n",
        "    // over as many blocks as necessary given\n",
        "    // the hardware limit on the number of threads per block\n",
        "    int threadsPerBlock = fmatrix_elements(A);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_multiplication_kernel<<< blocksPerGrid, threadsPerBlock >>>(A,f,B,C);\n",
        "    // check for errors\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    // wait for the kernel to finish\n",
        "    device_synchronize();\n",
        "}\n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix_mult.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Softmax\n",
        "\n",
        "We use __shared__ float sum[P.cols] to store the sum of each column, each sum is only calculated once and then stored."
      ],
      "metadata": {
        "id": "5M-PpUH6XFdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fmatrix_simple_softmax.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "__global__ \n",
        "void fmatrix_simple_softmax_kernel(fmatrix P, fmatrix Z) {\n",
        "    \n",
        "    __shared__ float sum[32];   // set max (P.cols) to 32\n",
        "\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        if (i == 0){\n",
        "            sum[j] = 0;\n",
        "            for (int k = 0; k < P.rows; ++k) {\n",
        "              sum[j] += exp(getfm(P,k,j));\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads(); \n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        getfm(Z,i,j) = exp(getfm(P,i,j)) / sum[j];\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_simple_softmax(fmatrix P, fmatrix Z) {\n",
        "    fmatrix_assert(P);\n",
        "    fmatrix_assert(Z);\n",
        "    assert(P.rows == Z.rows);\n",
        "    assert(P.cols == Z.cols);\n",
        "    int threadsPerBlock = fmatrix_elements(Z);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_simple_softmax_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, Z);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewWBBtBCXE1f",
        "outputId": "047f5c14-5e58-4ef1-e871-baa71c7fbe0c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix_simple_softmax.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stable Softmax\n",
        "\n",
        "We use __shared__ float sum[P.cols] to store the sum of each column, each sum is only calculated once and then stored. Same idea for __shared__ float max[]."
      ],
      "metadata": {
        "id": "zDT5XgNRjshL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fmatrix_stable_softmax.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "__global__ \n",
        "void fmatrix_stable_softmax_kernel(fmatrix P, fmatrix Z) {\n",
        "    \n",
        "    __shared__ float sum[32];\n",
        "    __shared__ float max[32];\n",
        "\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        if (i == 0){\n",
        "            max[j] = getfm(P,0,j);\n",
        "            for (int k = 0; k < P.rows; ++k) {\n",
        "                if (getfm(P,k,j) > max[j]) max[j] = getfm(P,k,j);\n",
        "            }\n",
        "            sum[j] = 0;\n",
        "            for (int k = 0; k < P.rows; ++k) {\n",
        "                sum[j] += exp(getfm(P,k,j) - max[j]);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        getfm(Z,i,j) = exp(getfm(P,i,j) - max[j]) / sum[j];\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_stable_softmax(fmatrix P, fmatrix Z) {\n",
        "    fmatrix_assert(P);\n",
        "    fmatrix_assert(Z);\n",
        "    assert(P.rows == Z.rows);\n",
        "    assert(P.cols == Z.cols);\n",
        "    int threadsPerBlock = fmatrix_elements(Z);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_stable_softmax_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, Z);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ZkZ_CJj1P3",
        "outputId": "71040b10-876e-49a4-cbf5-9d41d6539746"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix_stable_softmax.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute mu and sigma\n",
        "\n",
        "We use shared float sum[P.rows] to store the sum of each row, each sum is only calculated once and then stored. Same idea for shared float sigma."
      ],
      "metadata": {
        "id": "AwXyYEzjx-o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " compute the vectors 𝜇 and 𝜎 for a matrix X, the output is 2 matrix of size (X.rows,1), representing 𝜇 and 𝜎"
      ],
      "metadata": {
        "id": "etKp45O99ZWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fmatrix_compute.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "__global__ \n",
        "void fmatrix_compute_kernel(fmatrix P, fmatrix M_mu, fmatrix M_sigma) {\n",
        "    __shared__ float sum[32];\n",
        "    __shared__ float sigma[32];\n",
        "\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        if (j == 0){\n",
        "            for (int k = 0; k < P.cols; ++k) {\n",
        "              sum[i] += getfm(P,i,k);\n",
        "            }   \n",
        "        }\n",
        "        __syncthreads(); \n",
        "\n",
        "        if (j == 0){\n",
        "            float mu = sum[i] / P.cols;\n",
        "            float sum_square = 0;     \n",
        "            for (int k = 0; k < P.cols; ++k) {\n",
        "              sum_square += pow(getfm(P,i,k) - mu, 2);\n",
        "            }     \n",
        "            sigma[i] = sqrt(sum_square / P.cols);\n",
        "        }\n",
        "        __syncthreads(); \n",
        "        getfm(M_mu,i,j) = sum[i] / P.cols;\n",
        "        getfm(M_sigma,i,j) = sigma[i];\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_compute(fmatrix P, fmatrix mu, fmatrix sigma) {\n",
        "    int threadsPerBlock = fmatrix_elements(P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_compute_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, mu, sigma);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfRi_M0Yx9cg",
        "outputId": "f4eaf47d-0a7a-4415-95ed-3e1d779789bf"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix_compute.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize another matrix with given mu and sigma"
      ],
      "metadata": {
        "id": "mHzL7q1TOPOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fmatrix_normalize.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include <assert.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "__global__ \n",
        "void fmatrix_normalize_kernel(fmatrix P, fmatrix M_mu, fmatrix M_sigma) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        getfm(P, i, j) = (getfm(P, i, j) - getfm(M_mu, i, 1)) / getfm(M_sigma, i, 1);\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_normalize(fmatrix P, fmatrix mu, fmatrix sigma) {\n",
        "    int threadsPerBlock = fmatrix_elements(P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_normalize_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, mu, sigma);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fZAC0eL92z_",
        "outputId": "adb1493c-b043-4544-c096-f7cb0d0801c5"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix_normalize.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3ba7BDmf92Zh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_16DcZBAebJ"
      },
      "source": [
        "# Testing the Matrix Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test my simple softmax\n",
        "\n",
        "The range of possible values for the resulting elements of Z, is (0, 1) if they are computed perfectly. When the absolute value of elements of Z is not too big, our algorithm works well. However, if they are too large, then the results of exp operation will be out of bounds in programming, making it impossible to calculate, which is why we need to create a stable softmax later."
      ],
      "metadata": {
        "id": "qrqmxMH0c4E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_simple_softmax.cu\"\n",
        "#include \"fmatrix_add.cu\"\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    fmatrix M = fmatrix_create_random_on_device(3,5);\n",
        "    printf(\"original matrix:\\n\");\n",
        "    fmatrix_device_print(M);\n",
        "    fmatrix_simple_softmax(M, M);\n",
        "    printf(\"simple softmax:\\n\");\n",
        "    fmatrix_device_print(M);\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    fmatrix M1 = fmatrix_create_random_on_device(3,5);\n",
        "    printf(\"large matrix:\\n\");\n",
        "    fmatrix_add(M1,10000.0,M1);\n",
        "    fmatrix_device_print(M1);\n",
        "    fmatrix_simple_softmax(M1, M1);\n",
        "    printf(\"simple softmax large :\\n\");\n",
        "    fmatrix_device_print(M1);\n",
        "\n",
        "\n",
        "    fmatrix_free_on_device(&M);\n",
        "    fmatrix_free_on_device(&M1);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBnBsTQyc3ip",
        "outputId": "f98c4855-a522-4200-8a05-6b73fc81b964"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_35 -Wno-deprecated-gpu-targets -g -G -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ],
      "metadata": {
        "id": "TSeDmenOdTT1"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw3WFUH2f_It",
        "outputId": "43b014f1-a37f-47cb-830d-4dec767746d5"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original matrix:\n",
            "[\n",
            "0.480439,\t-0.686023,\t-0.544713,\t-0.624402,\t0.949322;\n",
            "-0.123098,\t-0.857265,\t-0.341283,\t0.830767,\t0.094939;\n",
            "0.034025,\t-0.074983,\t-0.711869,\t0.080250,\t0.306321\n",
            "]\n",
            "simple softmax:\n",
            "[\n",
            "0.457291,\t0.271373,\t0.325556,\t0.136830,\t0.512490;\n",
            "0.250080,\t0.228664,\t0.399001,\t0.586344,\t0.218088;\n",
            "0.292629,\t0.499963,\t0.275442,\t0.276826,\t0.269422\n",
            "]\n",
            "large matrix:\n",
            "[\n",
            "4804.867676,\t-6860.913574,\t-5447.679688,\t-6244.642578,\t9494.166992;\n",
            "-1231.100220,\t-8573.507812,\t-3413.171631,\t8308.500000,\t949.480103;\n",
            "340.287140,\t-749.907410,\t-7119.405273,\t802.576904,\t3063.511963\n",
            "]\n",
            "simple softmax large :\n",
            "[\n",
            "nan,\tnan,\tnan,\t0.000000,\tnan;\n",
            "0.000000,\tnan,\tnan,\tnan,\tnan;\n",
            "nan,\tnan,\tnan,\tnan,\tnan\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the absolute value of elements of Z is not too big, our algorithm works well. However, if they are too large, then the results of exp operation will be out of bounds, making it impossible to calculate, which is why we need to create a stable softmax next."
      ],
      "metadata": {
        "id": "2h5xOcKnoSgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test my stable softmax\n",
        "\n",
        "The stable softmax is identical to softmax over the real numbers, but is more stable in floating point since it avoids overflow and improves the precision. Now, when the elements of Z are too large, the results of exp operation will not be out of bounds, and it's possible to calculate."
      ],
      "metadata": {
        "id": "_iBCyYjIlvwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_stable_softmax.cu\"\n",
        "#include \"fmatrix_add.cu\"\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    fmatrix M = fmatrix_create_random_on_device(3,5);\n",
        "    fmatrix_device_to_csv(\"matrix_stable_softmax_test.csv\",M);\n",
        "    printf(\"original matrix:\\n\");\n",
        "    fmatrix_device_print(M);\n",
        "    fmatrix_stable_softmax(M, M);\n",
        "    printf(\"stable softmax:\\n\");\n",
        "    fmatrix_device_print(M);\n",
        "    fmatrix_device_to_csv(\"result_matrix_stable_softmax_test.csv\",M);\n",
        "\n",
        "\n",
        "    \n",
        "    fmatrix M1 = fmatrix_create_random_on_device(3,5);\n",
        "    fmatrix_device_to_csv(\"large_matrix_stable_softmax_test.csv\",M1);\n",
        "    printf(\"large matrix:\\n\");\n",
        "    fmatrix_add(M1,50.0,M1);\n",
        "    fmatrix_device_print(M1);\n",
        "    fmatrix_stable_softmax(M1, M1);\n",
        "    printf(\"stable softmax large:\\n\");\n",
        "    fmatrix_device_print(M1);\n",
        "    fmatrix_device_to_csv(\"result_large_matrix_stable_softmax_test.csv\",M1);\n",
        "\n",
        "\n",
        "    fmatrix_free_on_device(&M);\n",
        "    fmatrix_free_on_device(&M1);\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2lgNvyPlo3N",
        "outputId": "0743e054-756d-4e21-b827-d8c6b91efc62"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_35 -Wno-deprecated-gpu-targets -g -G -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ],
      "metadata": {
        "id": "4JiuaklumEoC"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSxDId2GmXmi",
        "outputId": "42cdec4d-fee2-4d9a-bed2-2b26991b778c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original matrix:\n",
            "[\n",
            "0.480439,\t-0.686023,\t-0.544713,\t-0.624402,\t0.949322;\n",
            "-0.123098,\t-0.857265,\t-0.341283,\t0.830767,\t0.094939;\n",
            "0.034025,\t-0.074983,\t-0.711869,\t0.080250,\t0.306321\n",
            "]\n",
            "stable softmax:\n",
            "[\n",
            "0.457291,\t0.271373,\t0.325556,\t0.136830,\t0.512490;\n",
            "0.250080,\t0.228664,\t0.399001,\t0.586344,\t0.218088;\n",
            "0.292629,\t0.499963,\t0.275442,\t0.276826,\t0.269422\n",
            "]\n",
            "large matrix:\n",
            "[\n",
            "24.502373,\t-34.987164,\t-27.780390,\t-31.844492,\t48.415409;\n",
            "-6.277984,\t-43.720516,\t-17.405436,\t42.369114,\t4.841865;\n",
            "1.735291,\t-3.824145,\t-36.305336,\t4.092733,\t15.622348\n",
            "]\n",
            "stable softmax large:\n",
            "[\n",
            "1.000000,\t0.000000,\t0.000031,\t0.000000,\t1.000000;\n",
            "0.000000,\t0.000000,\t0.999969,\t1.000000,\t0.000000;\n",
            "0.000000,\t1.000000,\t0.000000,\t0.000000,\t0.000000\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test my compute mu and sigma"
      ],
      "metadata": {
        "id": "_Vn-fPdVz9RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"fmatrix_compute.cu\"\n",
        "#include \"fmatrix_normalize.cu\"\n",
        "#include \"fmatrix_add.cu\"\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    fmatrix P = fmatrix_create_random_on_device(3,5);\n",
        "    fmatrix Q = fmatrix_create_random_on_device(3,5);\n",
        "    fmatrix_add(Q,0.2,Q);\n",
        "\n",
        "    fmatrix mu = fmatrix_create_random_on_device(3,1);\n",
        "    fmatrix sigma = fmatrix_create_random_on_device(3,1);\n",
        "\n",
        "    printf(\"original matrix:\\n\");\n",
        "    fmatrix_device_print(P);\n",
        "    fmatrix_device_to_csv(\"matrix_normalize.csv\",P);\n",
        "    \n",
        "    fmatrix_compute(P, mu, sigma);\n",
        "    printf(\"mu:\\n\");\n",
        "    fmatrix_device_to_csv(\"matrix_mu.csv\",mu);\n",
        "    fmatrix_device_print(mu);\n",
        "    printf(\"sigma:\\n\");\n",
        "    fmatrix_device_to_csv(\"matrix_sigma.csv\",sigma);\n",
        "    fmatrix_device_print(sigma);\n",
        "    \n",
        "\n",
        "    printf(\"second matrix which is to be normalized:\\n\");\n",
        "    fmatrix_device_print(Q);\n",
        "    fmatrix_device_to_csv(\"matrix_to_be_normalized.csv\",Q);\n",
        "    fmatrix_normalize(Q, mu, sigma);\n",
        "    printf(\"after normalization:\\n\");\n",
        "    fmatrix_device_to_csv(\"matrix_after_normalize.csv\",Q);\n",
        "    fmatrix_device_print(Q);\n",
        "\n",
        "    fmatrix_free_on_device(&P);\n",
        "    fmatrix_free_on_device(&mu);\n",
        "    fmatrix_free_on_device(&sigma);\n",
        "    fmatrix_free_on_device(&Q);\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZZPNZD70BCG",
        "outputId": "afaece7e-dbf7-49cc-d9cc-f5a37bb5a0bc"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_35 -Wno-deprecated-gpu-targets -g -G -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include test.cu fmatrix.cu cuda_stuff.cu"
      ],
      "metadata": {
        "id": "gD_o8hrW0kuJ"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMh8sgk71gcq",
        "outputId": "381ff292-bc8d-441c-ee08-24f3542841e1"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original matrix:\n",
            "[\n",
            "0.480439,\t-0.686023,\t-0.544713,\t-0.624402,\t0.949322;\n",
            "-0.123098,\t-0.857265,\t-0.341283,\t0.830767,\t0.094939;\n",
            "0.034025,\t-0.074983,\t-0.711869,\t0.080250,\t0.306321\n",
            "]\n",
            "mu:\n",
            "[\n",
            "-0.085076;\n",
            "-0.079188;\n",
            "-0.073251\n",
            "]\n",
            "sigma:\n",
            "[\n",
            "0.671277;\n",
            "0.553961;\n",
            "0.342631\n",
            "]\n",
            "second matrix which is to be normalized:\n",
            "[\n",
            "0.576526,\t-0.823227,\t-0.653656,\t-0.749282,\t1.139186;\n",
            "-0.147717,\t-1.028718,\t-0.409540,\t0.996920,\t0.113926;\n",
            "0.040830,\t-0.089980,\t-0.854243,\t0.096300,\t0.367585\n",
            "]\n",
            "after normalization:\n",
            "[\n",
            "0.985588,\t-1.099624,\t-0.847014,\t-0.989468,\t1.823781;\n",
            "-0.123708,\t-1.714074,\t-0.596345,\t1.942571,\t0.348606;\n",
            "0.332958,\t-0.048824,\t-2.279393,\t0.494850,\t1.286619\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzlIT2mInHO9"
      },
      "source": [
        "## My own comparison between my stable softmax and Numpy\n",
        "\n",
        "Exactly the same results with numpy, created functions for softmax are correct."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# here the softmax is row-major, so we should (1)m = m.T, (2)m = softmax(m), (3) m = m.T\n",
        "\n",
        "def softmax(x):\n",
        "    x_row_max = x.max(axis=-1)\n",
        "    x_row_max = x_row_max.reshape(list(x.shape)[:-1]+[1])\n",
        "    x = x - x_row_max\n",
        "    x_exp = np.exp(x)\n",
        "    x_exp_row_sum = x_exp.sum(axis=-1).reshape(list(x.shape)[:-1]+[1])\n",
        "    softmax = x_exp / x_exp_row_sum\n",
        "    return softmax\n",
        "\n",
        "m = np.loadtxt(\"matrix_stable_softmax_test.csv\",delimiter=',',ndmin=2)\n",
        "print(\"Test our stable softmax:\")\n",
        "print(\"\\ninitial matrix:\")\n",
        "print(m)\n",
        "\n",
        "m = np.transpose(m)\n",
        "m = softmax(m)\n",
        "m = np.transpose(m)\n",
        "print(\"\\nnumpy softmax result:\")\n",
        "print(m)\n",
        "\n",
        "my_m = np.loadtxt(\"result_matrix_stable_softmax_test.csv\",delimiter=',',ndmin=2)\n",
        "print(\"\\nmy stable softmax matrix:\")\n",
        "print(my_m)\n",
        "\n",
        "result = np.allclose(m, my_m)\n",
        "print(\"\\nTest passed?\", result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eedM1CNsvDj1",
        "outputId": "d82592ef-8fed-4135-ff1d-40cd15bd05eb"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test our stable softmax:\n",
            "\n",
            "initial matrix:\n",
            "[[ 0.48043871 -0.68602276 -0.5447135  -0.62440181  0.94932175]\n",
            " [-0.12309772 -0.857265   -0.34128302  0.83076692  0.09493852]\n",
            " [ 0.03402531 -0.07498324 -0.71186936  0.08024967  0.30632055]]\n",
            "\n",
            "numpy softmax result:\n",
            "[[0.45729057 0.27137315 0.32555605 0.13682983 0.51248993]\n",
            " [0.25008043 0.22866374 0.39900148 0.58634407 0.21808782]\n",
            " [0.292629   0.49996311 0.27544247 0.2768261  0.26942225]]\n",
            "\n",
            "my stable softmax matrix:\n",
            "[[0.45729053 0.27137315 0.32555604 0.13682982 0.51248991]\n",
            " [0.25008041 0.22866376 0.39900148 0.58634406 0.21808782]\n",
            " [0.292629   0.4999631  0.27544248 0.27682611 0.26942223]]\n",
            "\n",
            "Test passed? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My own comparison between my normalization and Numpy\n",
        "\n",
        "Exactly the same results with numpy, created functions for normalization are correct."
      ],
      "metadata": {
        "id": "qxTmeBoqE6Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# here the softmax is row-major, so we should (1)m = m.T, (2)m = softmax(m), (3) m = m.T\n",
        "\n",
        "def compute(x):\n",
        "    x_row_sum = x.sum(axis=-1).reshape(list(x.shape)[:-1]+[1])\n",
        "    x_row_mean = x_row_sum / x.shape[1]\n",
        "    mu = x_row_mean\n",
        "    x_row_square = np.square(x - x_row_mean)\n",
        "    x_row_square_sum = x_row_square.sum(axis=-1).reshape(list(x.shape)[:-1]+[1])\n",
        "    sigma = np.sqrt( x_row_square_sum / x.shape[1])\n",
        "    return mu, sigma\n",
        "\n",
        "def normalize(x, mu, sigma):\n",
        "    x = (x - mu) / sigma\n",
        "    return x\n",
        "\n",
        "print(\"Test our normalization:\\n\")\n",
        "\n",
        "m = np.loadtxt(\"matrix_normalize.csv\",delimiter=',',ndmin=2)\n",
        "print(\"fist matrix(used to calculate mu and sigma):\")\n",
        "print(m)\n",
        "\n",
        "q = np.loadtxt(\"matrix_to_be_normalized.csv\",delimiter=',',ndmin=2)\n",
        "print(\"second matrix(to be normalized):\")\n",
        "print(q)\n",
        "\n",
        "mu, sigma = compute(m)\n",
        "print(\"\\nnumpy compute result:\")\n",
        "print(\"mu of first matrix:\")\n",
        "print(mu)\n",
        "print(\"sigma of first matrix:\")\n",
        "print(sigma)\n",
        "print(\"applying normalization on the secon matrix using mu and sigma of the first matrix:\")\n",
        "q_n = normalize(q, mu, sigma)\n",
        "print(q_n)\n",
        "\n",
        "\n",
        "print(\"\\nmy normalization function result:\")\n",
        "mu_1 = np.loadtxt(\"matrix_mu.csv\",delimiter=',',ndmin=2)\n",
        "print(\"mu of first matrix:\")\n",
        "print(mu_1)\n",
        "sigma_1 = np.loadtxt(\"matrix_sigma.csv\",delimiter=',',ndmin=2)\n",
        "print(\"sigma of first matrix:\")\n",
        "print(sigma_1)\n",
        "q_n_1 = np.loadtxt(\"matrix_after_normalize.csv\",delimiter=',',ndmin=2)\n",
        "print(\"applying normalization on the secon matrix using mu and sigma of the first matrix:\")\n",
        "print(q_n_1)\n",
        "\n",
        "\n",
        "\n",
        "result1 = np.allclose(mu, mu_1)\n",
        "print(\"\\nmu test passed?\", result1)\n",
        "result2 = np.allclose(sigma, sigma_1)\n",
        "print(\"sigma test passed?\", result2)\n",
        "result3 = np.allclose(q_n, q_n_1)\n",
        "print(\"second matrix after normalize test passed?\", result3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JpiVQJv_8c9",
        "outputId": "b63e8356-7bfa-4444-acef-7ff2d458c66e"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test our normalization:\n",
            "\n",
            "fist matrix(used to calculate mu and sigma):\n",
            "[[ 0.48043871 -0.68602276 -0.5447135  -0.62440181  0.94932175]\n",
            " [-0.12309772 -0.857265   -0.34128302  0.83076692  0.09493852]\n",
            " [ 0.03402531 -0.07498324 -0.71186936  0.08024967  0.30632055]]\n",
            "second matrix(to be normalized):\n",
            "[[ 0.57652646 -0.82322729 -0.65365618 -0.74928218  1.13918614]\n",
            " [-0.14771727 -1.02871799 -0.40953964  0.99692029  0.11392622]\n",
            " [ 0.04083037 -0.08997989 -0.85424322  0.0962996   0.36758465]]\n",
            "\n",
            "numpy compute result:\n",
            "mu of first matrix:\n",
            "[[-0.08507552]\n",
            " [-0.07918806]\n",
            " [-0.07325141]]\n",
            "sigma of first matrix:\n",
            "[[0.67127663]\n",
            " [0.55396095]\n",
            " [0.34263147]]\n",
            "applying normalization on the secon matrix using mu and sigma of the first matrix:\n",
            "[[ 0.9855877  -1.09962381 -0.84701394 -0.98946788  1.82378116]\n",
            " [-0.12370765 -1.71407377 -0.59634452  1.94257077  0.3486063 ]\n",
            " [ 0.3329577  -0.04882351 -2.27939307  0.49484951  1.2866187 ]]\n",
            "\n",
            "my normalization function result:\n",
            "mu of first matrix:\n",
            "[[-0.08507552]\n",
            " [-0.07918806]\n",
            " [-0.07325141]]\n",
            "sigma of first matrix:\n",
            "[[0.67127663]\n",
            " [0.55396092]\n",
            " [0.34263146]]\n",
            "applying normalization on the secon matrix using mu and sigma of the first matrix:\n",
            "[[ 0.98558766 -1.0996238  -0.84701395 -0.98946792  1.82378113]\n",
            " [-0.12370765 -1.7140739  -0.59634459  1.94257081  0.34860632]\n",
            " [ 0.33295771 -0.04882352 -2.2793932   0.49484953  1.28661883]]\n",
            "\n",
            "mu test passed? True\n",
            "sigma test passed? True\n",
            "second matrix after normalize test passed? True\n"
          ]
        }
      ]
    }
  ]
}