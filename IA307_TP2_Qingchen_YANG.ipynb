{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA307_TP2_Qingchen_YANG",
      "provenance": [],
      "collapsed_sections": [
        "iwd38XvZK2sy",
        "cweMlOB0L4mG",
        "xsKLTK8ELOdN",
        "9pWS3hlAecOc",
        "I6G22rDKR3rv",
        "ujbwQZlQDqWF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IA307_TP2_Qingchen_YANG"
      ],
      "metadata": {
        "id": "fSK0eafO2jKG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwd38XvZK2sy"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ewR7_sHJ00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fc9d7b-f7f9-48bb-a395-293049a1f55b"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn2s4LVCHL0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e80da45-d3d1-4d48-fedb-73afc33a4571"
      },
      "source": [
        "!pip install --upgrade git+git://github.com/frehseg/nvcc4jupyter.git"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+git://github.com/frehseg/nvcc4jupyter.git\n",
            "  Cloning git://github.com/frehseg/nvcc4jupyter.git to /tmp/pip-req-build-dus3392y\n",
            "  Running command git clone -q git://github.com/frehseg/nvcc4jupyter.git /tmp/pip-req-build-dus3392y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoKiUawdHMGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee33b958-b631-43b1-f5e1-0cf1a211814e"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIyq5vPjxKc"
      },
      "source": [
        "Based on the lecture at https://sites.google.com/site/frehseg/teaching/ia307"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cweMlOB0L4mG"
      },
      "source": [
        "# Provided Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TEvrWMJ_a"
      },
      "source": [
        "## CUDA Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-lgwhE1N5_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d518693b-c645-4edc-82b0-5733898d37a8"
      },
      "source": [
        "%%writefile cuda_stuff.cuh\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#ifndef cuda_stuff_H\n",
        "#define cuda_stuff_H\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   nb_rows = number of rows */\n",
        "#define IDX2C(i,j,nb_rows) (((j)*(nb_rows))+(i))\n",
        " \n",
        "//MACRO TO DEBUGG CUDA FUNCTIONS\n",
        "/** Error checking,\n",
        " *  taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess) \n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "/** Error checking for use with CUDA Dynamic Parallelism */\n",
        "/*\n",
        "#define cdpErrchk(ans) { cdpAssert((ans), __FILE__, __LINE__); }\n",
        "__device__ void cdpAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      printf(\"GPU kernel assert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) assert(0);\n",
        "   }\n",
        "}\n",
        "*/\n",
        "void device_synchronize();\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda_stuff.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivrxLaYOYPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba27298-840e-46e1-bcf1-b4655ced0f01"
      },
      "source": [
        "%%writefile cuda_stuff.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "\n",
        "void device_synchronize(){\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda_stuff.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsEMpauK8lW"
      },
      "source": [
        "## fmatrix Matrix Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A97U902HMog4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7b0a0a-f141-4a82-8e56-c163c70ff514"
      },
      "source": [
        "%%writefile fmatrix.cuh\n",
        "#ifndef fmatrices_H\n",
        "#define fmatrices_H\n",
        "#include \"cuda_stuff.cuh\" // for IDX2C\n",
        "\n",
        "typedef struct {\n",
        "    float* data;\n",
        "    int cols;\n",
        "    int rows;\n",
        "} fmatrix;\n",
        "\n",
        "/* Access element (i,j) of matrix mat */\n",
        "#define getfm(mat,i,j) (mat.data[IDX2C(i,j,mat.rows)])\n",
        "\n",
        "\n",
        "int fmatrix_elements(fmatrix mat);\n",
        "int fmatrix_size(fmatrix mat);\n",
        "/** Assert that the matrix is coherent: all fields nonzero. */\n",
        "void fmatrix_assert(fmatrix mat);\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols);\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols);\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device);\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device);\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_host);\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host);\n",
        "void fmatrix_free_on_host(fmatrix* mat);\n",
        "void fmatrix_free_on_device(fmatrix* mat);\n",
        "\n",
        "/** Create a matrix representing columns [a,b) of M. \n",
        " *  Note that the new matrix points into the\n",
        " *  data of M. The data is not copied to a new location.\n",
        " */\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the host. \n",
        " *  If nb<0, print all rows. \n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "void fmatrix_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the device. \n",
        " *  If nb<0, print all rows. \n",
        " *\n",
        " *  This version copies the matrix to host first.\n",
        " */\n",
        "void fmatrix_device_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGwZ36ifWQ-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3d32e1-1804-42fa-a43f-1daf7f24ceb6"
      },
      "source": [
        "%%writefile fmatrix.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "int fmatrix_elements(fmatrix mat) {\n",
        "     return mat.cols*mat.rows;\n",
        "}\n",
        "\n",
        "int fmatrix_size(fmatrix mat) {\n",
        "     return fmatrix_elements(mat) * sizeof(mat.data[0]);\n",
        "}\n",
        "\n",
        "void fmatrix_assert(fmatrix mat) {\n",
        "    assert(mat.data);\n",
        "    assert(mat.cols);\n",
        "    assert(mat.rows);\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    mat.data = (float*)malloc(fmatrix_size(mat)); \n",
        "    assert(mat.data);\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    gpuErrchk( \n",
        "        cudaMalloc((void **)&(mat.data), fmatrix_size(mat)) \n",
        "    );\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk( \n",
        "        cudaMemcpy( mat_device.data, mat_host.data, \n",
        "                   fmatrix_size(mat_host), \n",
        "                   cudaMemcpyHostToDevice \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_host.data, mat_device.data,  \n",
        "                   fmatrix_size(mat_device), \n",
        "                   cudaMemcpyDeviceToHost \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_device);\n",
        "    fmatrix mat_host = fmatrix_create_on_host(mat_device.rows, mat_device.cols);\n",
        "    fmatrix_data_to_host(mat_host,mat_device);\n",
        "    return mat_host;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix mat_device = fmatrix_create_on_device(mat_host.rows, mat_host.cols);\n",
        "    fmatrix_data_to_device(mat_host,mat_device);\n",
        "    return mat_device;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  free(mat->data);\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_device(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  gpuErrchk(cudaFree(mat->data));\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b) {\n",
        "    fmatrix_assert(M);  \n",
        "    fmatrix A = { \n",
        "        .data = &getfm(M,0,a),  \n",
        "        .cols = b-a,\n",
        "        .rows = M.rows \n",
        "    };\n",
        "    fmatrix_assert(A);  \n",
        "    return A;\n",
        "}\n",
        "\n",
        "\n",
        "__host__\n",
        "__device__\n",
        "void fmatrix_print(fmatrix mat, int nb){\n",
        "    if (nb<0 || nb > mat.rows) {\n",
        "        nb = mat.rows;\n",
        "    }\n",
        "    printf(\"[\\n\");\n",
        "    for (int i = 0 ; i < nb; i++){\n",
        "      for (int j = 0 ; j<mat.cols; j++){\n",
        "        printf(\"%f\", getfm(mat,i,j));\n",
        "        if (j+1<mat.cols) {\n",
        "          printf(\",\\t\");\n",
        "        }\n",
        "      }\n",
        "      if (i+1<nb) {\n",
        "        printf(\";\\n\");\n",
        "      }\n",
        "    }\n",
        "    if (nb < mat.rows) {\n",
        "      printf(\"\\n...\\n\");\n",
        "    }\n",
        "  printf(\"\\n]\\n\");\n",
        "}\n",
        "\n",
        "void fmatrix_device_print(fmatrix mat, int nb){\n",
        "   // allocate copy\n",
        "   fmatrix tmp = fmatrix_copy_to_host(mat);\n",
        "   fmatrix_print(tmp,nb);\n",
        "   fmatrix_free_on_host(&tmp);\n",
        "}\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fmatrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsKLTK8ELOdN"
      },
      "source": [
        "## Data I/O"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD7rmOBmWfsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30475cdd-e559-4f88-a892-2261575f9f06"
      },
      "source": [
        "%%writefile read_csv.cuh\n",
        "#include <cuda_runtime.h>\n",
        "#ifndef read_csv_H\n",
        "#define read_csv_H\n",
        "\n",
        "void read_csv(const char* filename, float* data_array,int nbrow,int nbcol);\n",
        "\n",
        "#endif"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting read_csv.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeedFsZ_WQx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7fecd3-01cd-424a-cd17-ccf77e51884c"
      },
      "source": [
        "%%writefile read_csv.cu\n",
        "\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <fstream>\n",
        "\n",
        "#include \"read_csv.cuh\"\n",
        "#include \"cuda_stuff.cuh\" // for matrix indexing\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Functions for reading the dataset from a file\n",
        "/////////////////////////////////////////////////////////\n",
        "\n",
        "/* Read a csv file with a given number of rows and columns */\n",
        "void read_csv(const char* filename, float* data_array,int nbrow,int nbcol) {\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  double ioTemp;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "  int row_count = 0;\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "      // read the headers (and discard)\n",
        "\t\t\tgetline(infile, row_as_string, '\\n');\n",
        "      cout << \"headers: \" << row_as_string << \"!\" << std::endl;\n",
        "      for(int i = 0; i < nbrow; i++){\n",
        "  \t\t\tgetline(infile, row_as_string, '\\n');\n",
        "        // cout << \"read line \" << row_as_string << \"!\" << std::endl;\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "\t\t\t  for(int j = 0; j < nbcol; j++){\n",
        "          getline(line_stream, value, ',');\n",
        "\t\t\t\t\tioTemp = strtod(value.c_str(), NULL); \n",
        "          // cout << \"(\"<<i<<\",\"<<j<<\") = \"<< ioTemp << std::endl;\n",
        "\n",
        "\t\t\t\t\tdata_array[IDX2C(i,j,nbrow)] = ioTemp;\n",
        "\n",
        "\t\t\t\t}\n",
        "        ++row_count;\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "    cout << \"Read \" << row_count << \" rows.\" << std::endl;\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting read_csv.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8ilQdYYroU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950b5975-3dd2-4b44-b593-6dc308ad0c7c"
      },
      "source": [
        "%%writefile preprocess_data.cuh\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#ifndef preprocess_data_H\n",
        "#define preprocess_data_H\n",
        "\n",
        "void get_inputs_and_labels(float* data_array, float** input_array, float** label_array, int nbrows, int nbcols, int nb_inputs, int nb_labels );\n",
        "\n",
        "#endif"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocess_data.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaeUdw_KYaCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f36e8db-eb70-4cbf-eecc-be04e11c8ac8"
      },
      "source": [
        "%%writefile preprocess_data.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <fstream>\n",
        "\n",
        "/*Matrix multiplication functions and other auxiliary functions*/\n",
        "#include \"preprocess_data.cuh\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   ld = number of rows \n",
        "   Example of use: a[IDX2C(0, 1, 50)] */\n",
        "#define IDX2C(i,j,ld) (((j)*(ld))+(i))\n",
        "\n",
        "//Number of thread per block\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "/* Constants for housing data set */\n",
        "#define data_columns  (9)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Number of rows in arrays to print for debugging\n",
        "/////////////////////////////////////////////////////////\n",
        "#define print_rows (10)\n",
        "/////////////////////////////////////////////////////////\n",
        "// Functions for preprocessing the data set\n",
        "/////////////////////////////////////////////////////////\n",
        "\n",
        "/* Split data into inputs and labels. Allocated memory for inputs and labels.\n",
        "   Since cuBLAS is column major, each input is in a column.\n",
        "   We also add 1.0 as first element to each input vector.\n",
        "*/\n",
        "void get_inputs_and_labels(float* data_array, float** input_array, float** label_array, int nbrows, int nbcols, int nb_inputs, int nb_labels ) {\n",
        "    // The inputs are the first nbrows-1 columns.\n",
        "    // The labels are the last column (index nbrows-1), booleanized\n",
        "    // by the condition >= above_threshold\n",
        "    *input_array = (float *)malloc(nbrows * nb_inputs * sizeof(float));    \n",
        "    *label_array = (float *)malloc(nbrows * nb_labels * sizeof(float));    \n",
        "    //cout << &input_array << \" and \"<< &label_array << \" data \" << data_array << std::endl;\n",
        "    cout << \"Allocated memory for inputs: \" << nbrows << \" rows, \"<< nb_inputs << \" columns.\" << std::endl;\n",
        "    cout << \"Allocated memory for labels: \" << nbrows << \" rows, \"<< nb_labels << \" columns.\" << std::endl;\n",
        "\n",
        "    // Copy the data to X\n",
        "    for(int i = 0; i < nbrows; i++){\n",
        "      // Set the first element of each x to 1  \n",
        "      (*input_array)[IDX2C(0,i,nb_inputs)] = 1.0;\n",
        "      // Copy the rest of x\n",
        "\t\t\tfor(int j = 1; j < nb_inputs; j++){\n",
        "\t\t\t\t(*input_array)[IDX2C(j,i,nb_inputs)] = data_array[IDX2C(i,j-1,nbrows)];\n",
        "\t\t\t}\n",
        "      float median_house_value = data_array[IDX2C(i,nbcols-1,nbrows)];\n",
        "      (*label_array)[IDX2C(0,i,nb_labels)] = 0.0;\n",
        "      (*label_array)[IDX2C(1,i,nb_labels)] = 0.0;\n",
        "      if (median_house_value >= above_threshold) {\n",
        "        (*label_array)[IDX2C(0,i,nb_labels)] = 1.0;\n",
        "      } else {\n",
        "        (*label_array)[IDX2C(1,i,nb_labels)] = 1.0;        \n",
        "      }\n",
        "\t\t}    \n",
        "    \n",
        "    // Show some entries for double checking\n",
        "    cout << \"Inputs (first \"<<print_rows<<\"):\" << std::endl;\n",
        "\t  for(int j = 0; j < nb_inputs; j++){\n",
        "      for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\t\tcout << (*input_array)[IDX2C(j,i,nb_inputs)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "    cout << \"Labels (first \"<<print_rows<<\"):\" << std::endl;\n",
        "    for(int j = 0; j < nb_labels; j++){\n",
        "      for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\t\tcout << (*label_array)[IDX2C(j,i,nb_labels)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "}"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocess_data.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHy3EAid05oA"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code That You Write"
      ],
      "metadata": {
        "id": "rR-9WFucUWLC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pWS3hlAecOc"
      },
      "source": [
        "## Classifier Math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK_tmB-xbZKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95430f0-9395-42bb-c666-c59e5cf597e9"
      },
      "source": [
        "%%writefile classifier_math.cuh\n",
        "#ifndef classifier_math_H\n",
        "#define classifier_math_H\n",
        "\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "/** Returns a random float between min and max (including). */\n",
        "float float_rand( float min, float max );\n",
        "\n",
        "/** Initialize W with Xavier's method,\n",
        " *  scaled by a. */\n",
        "void xavier_weight_init(float a, fmatrix W);\n",
        "\n",
        "/** Compute the softmax for each column of Z and store in P **/\n",
        "void softmax_col(fmatrix P,fmatrix Z); \n",
        "\n",
        "///////////////////////////////////\n",
        "// TO BE COMPLETED\n",
        "// ... add your matrix math here\n",
        "///////////////////////////////////\n",
        "\n",
        "\n",
        "//compute the transpose of P and store it in Z\n",
        "void fmatrix_transpose(fmatrix P, fmatrix Z);\n",
        "\n",
        "\n",
        "void fmatrix_add(fmatrix P,float a,fmatrix Y);\n",
        "\n",
        "void fmatrix_mult(fmatrix A, float f, fmatrix B, fmatrix C);\n",
        "void fmatrix_compute(fmatrix P, fmatrix mu, fmatrix sigma);\n",
        "void fmatrix_normalize(fmatrix P, fmatrix mu, fmatrix sigma);\n",
        "\n",
        "#endif"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting classifier_math.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgXwgv6Bbo-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e9b089-5d13-4fbd-fa3b-31c7632413ee"
      },
      "source": [
        "%%writefile classifier_math.cu\n",
        "#include \"classifier_math.cuh\"\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Auxiliary function\n",
        "/////////////////////////////////////////////////////////\n",
        "// generate random numbers in interval [min,max]\n",
        "float float_rand( float min, float max )\n",
        "{\n",
        "    float scale = rand() / (float) RAND_MAX; /* [0, 1.0] */\n",
        "    return min + scale * ( max - min );      /* [min, max] */\n",
        "}\n",
        "\n",
        "void xavier_weight_init(float a, fmatrix W){\n",
        "    for (int j = 0; j < W.rows  ; ++j) {\n",
        "      for (int i = 0; i < W.cols  ; ++i) {\n",
        "          getfm(W,j,i) = a * (1.0/sqrt(W.cols+W.rows)) * float_rand(-1.0,1.0);\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ \n",
        "void fmatrix_stable_softmax_kernel(fmatrix P, fmatrix Z) {\n",
        "    \n",
        "    __shared__ float sum[5000];\n",
        "    __shared__ float max[5000];\n",
        "\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / Z.rows;\n",
        "    int i = idx % Z.rows;\n",
        "    if (i < Z.rows && j < Z.cols ){\n",
        "        if (i == 0){\n",
        "            max[j] = getfm(Z,0,j);\n",
        "            for (int k = 0; k < Z.rows; ++k) {\n",
        "                if (getfm(Z,k,j) > max[j]) max[j] = getfm(Z,k,j);\n",
        "            }\n",
        "            sum[j] = 0;\n",
        "            for (int k = 0; k < Z.rows; ++k) {\n",
        "                sum[j] += exp(getfm(Z,k,j) - max[j]);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    if (i < Z.rows && j < Z.cols ){\n",
        "        getfm(P,i,j) = exp(getfm(Z,i,j) - max[j]) / sum[j];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void softmax_col(fmatrix P,fmatrix Z) {\n",
        "    assert(P.cols==Z.cols);\n",
        "    assert(P.rows==Z.rows);\n",
        "    \n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    // ... compute the softmax here ...\n",
        "    ///////////////////////////////////\n",
        "\n",
        "    fmatrix_assert(P);\n",
        "    fmatrix_assert(Z);\n",
        "    int threadsPerBlock = fmatrix_elements(Z);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_stable_softmax_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, Z);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    device_synchronize();\n",
        "}\n",
        "\n",
        "\n",
        "__global__ \n",
        "void fmatrix_transpose_kernel(fmatrix P, fmatrix Z) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    getfm(Z,j,i) = getfm(P,i,j);\n",
        "    __syncthreads(); \n",
        "}\n",
        "\n",
        "void fmatrix_transpose(fmatrix P, fmatrix Z) {\n",
        "    fmatrix_assert(P);\n",
        "    fmatrix_assert(Z);\n",
        "    int threadsPerBlock = fmatrix_elements(Z);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_transpose_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, Z);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}\n",
        "\n",
        "\n",
        "__global__ \n",
        "void fmatrix_add_kernel(fmatrix P,float a,fmatrix Y) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        getfm(P,i,j) += a*getfm(Y,i,j);\n",
        "    }\n",
        "}\n",
        "\n",
        "/** Compute P = P + a*Y */\n",
        "void fmatrix_add(fmatrix P,float a,fmatrix Y) {\n",
        "    fmatrix_assert(P);\n",
        "    fmatrix_assert(Y);\n",
        "    int threadsPerBlock = fmatrix_elements(P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_add_kernel<<< blocksPerGrid, threadsPerBlock >>>(P,a,Y);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}\n",
        "\n",
        "__global__\n",
        "void fmatrix_multiplication_kernel(fmatrix A, float f, fmatrix B, fmatrix C) {\n",
        "    // Each thread multiplies one row of B with one column of C\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / A.rows;\n",
        "    int i = idx % A.rows;\n",
        "    if (i < A.rows && j < A.cols ){\n",
        "        getfm(A,i,j) = 0.0;\n",
        "        for (int k = 0; k < B.cols; ++k) {\n",
        "          getfm(A,i,j) += f*getfm(B,i,k)*getfm(C,k,j);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Compute A = f*B*C */\n",
        "void fmatrix_mult(fmatrix A, float f, fmatrix B, fmatrix C) {\n",
        "    // First let's check for errors in the argument M.\n",
        "    // This can help a LOT when debugging.\n",
        "    // A,B,C need to have nonzero pointers etc.\n",
        "    // fmatrix_assert(A);\n",
        "    // fmatrix_assert(B);\n",
        "    // fmatrix_assert(C);\n",
        "    \n",
        "    // take one thread per element, and distribute\n",
        "    // over as many blocks as necessary given\n",
        "    // the hardware limit on the number of threads per block\n",
        "    int threadsPerBlock = fmatrix_elements(A);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_multiplication_kernel<<< blocksPerGrid, threadsPerBlock >>>(A,f,B,C);\n",
        "    // check for errors\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    // wait for the kernel to finish\n",
        "    device_synchronize();\n",
        "}\n",
        "\n",
        "\n",
        "__global__ \n",
        "void fmatrix_compute_kernel(fmatrix P, fmatrix M_mu, fmatrix M_sigma) {\n",
        "    __shared__ float sum[5000];\n",
        "    __shared__ float sigma[5000];\n",
        "\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        if (j == 0){\n",
        "            for (int k = 0; k < P.cols; ++k) {\n",
        "              sum[i] += getfm(P,i,k);\n",
        "            }   \n",
        "        }\n",
        "        __syncthreads(); \n",
        "\n",
        "        if (j == 0){\n",
        "            float mu = sum[i] / P.cols;\n",
        "            float sum_square = 0;     \n",
        "            for (int k = 0; k < P.cols; ++k) {\n",
        "              sum_square += pow(getfm(P,i,k) - mu, 2);\n",
        "            }     \n",
        "            sigma[i] = sqrt(sum_square / P.cols);\n",
        "        }\n",
        "        __syncthreads(); \n",
        "        if (i < M_mu.rows && j < M_mu.cols){\n",
        "            getfm(M_mu,i,j) = sum[i] / P.cols;\n",
        "            getfm(M_sigma,i,j) = sigma[i];\n",
        "            \n",
        "        }\n",
        "        \n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_compute(fmatrix P, fmatrix mu, fmatrix sigma) {\n",
        "    int threadsPerBlock = fmatrix_elements(P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_compute_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, mu, sigma);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}\n",
        "\n",
        "\n",
        "__global__ \n",
        "void fmatrix_normalize_kernel(fmatrix P, fmatrix M_mu, fmatrix M_sigma) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        getfm(P, i, j) = (getfm(P, i, j) - getfm(M_mu, i, 0)) / (getfm(M_sigma, i, 0));\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_normalize(fmatrix P, fmatrix mu, fmatrix sigma) {\n",
        "    int threadsPerBlock = fmatrix_elements(P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_normalize_kernel<<< blocksPerGrid, threadsPerBlock >>>(P, mu, sigma);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting classifier_math.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6G22rDKR3rv"
      },
      "source": [
        "## Evaluating Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd7Vmzo72hpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea5225e-9af7-47b1-d26e-ebb4d270393c"
      },
      "source": [
        "%%writefile evaluate_accuracy.cuh\n",
        "\n",
        "/** Evaluate the accuracy of a linear classifier with D x M weight\n",
        " *  matrix W, using D x N input data X and M x N output labels Y.\n",
        " *  Z is a temporary matrix with dimensions M x N,\n",
        " *  which must be previously allocated.\n",
        " */\n",
        "float evaluate_accuracy(fmatrix d_W,fmatrix d_X,fmatrix d_Y,fmatrix d_Z);\n",
        "\n",
        "/** Compute the logloss given M x N matrices of \n",
        " *  probabilities P and output labels Y\n",
        " *  and stores it in J.\n",
        " *  J is a matrix with dimensions 1 x 1,\n",
        " *  which must be previously allocated.\n",
        " *  logloss = sum_j -Y(j,k)*log(P(j,k))\n",
        " */\n",
        "float evaluate_logloss(fmatrix d_P,fmatrix d_Y);"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate_accuracy.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0Z-9B4a2dwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58757a18-112a-4d59-aafc-bbceb9a146aa"
      },
      "source": [
        "%%writefile evaluate_accuracy.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"classifier_math.cuh\"\n",
        "#include <assert.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__ \n",
        "void evaluate_accuracy_kernel(fmatrix d_Y,fmatrix d_Z,int* count) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    if (idx < d_Z.cols){\n",
        "        float z_max = getfm(d_Z,0,idx);\n",
        "        int i_max = 0;\n",
        "        for (int i = 1; i < d_Z.rows; ++i) {\n",
        "          if (getfm(d_Z,i,idx)>z_max) {\n",
        "                z_max = getfm(d_Z,i,idx);\n",
        "                i_max = i;\n",
        "          }\n",
        "        }\n",
        "      if (getfm(d_Y,i_max,idx)>=0.5f) {\n",
        "          atomicAdd(count,1);\n",
        "      }\n",
        "    }    \n",
        "}\n",
        "\n",
        "float evaluate_accuracy(fmatrix d_W,fmatrix d_X,fmatrix d_Y,fmatrix d_Z) {\n",
        "    assert(d_Y.cols == d_Z.cols);\n",
        "    assert(d_Y.rows == d_Z.rows);\n",
        "\n",
        "  //////////////////////////////////////////\n",
        "  // 1. compute Z = W^T X\n",
        "  // --> each column of Z corresponds to one input\n",
        "  //////////////////////////////////////////\n",
        "\n",
        "  /*********************************\n",
        "  / TO BE COMPLETED\n",
        "  / ... compute Z = W^T X here ...\n",
        "  **********************************/\n",
        "\n",
        "\n",
        "  fmatrix d_W_T = fmatrix_create_on_device(d_W.cols,d_W.rows);\n",
        "  fmatrix_transpose(d_W, d_W_T);\n",
        "  fmatrix_mult(d_Z, 1, d_W_T, d_X);\n",
        "  fmatrix_free_on_device(&d_W_T);\n",
        "\n",
        "\n",
        "  //////////////////////////////////////////\n",
        "  // 2. For each column z of Z, \n",
        "  // find argmax_k z_k\n",
        "  //////////////////////////////////////////\n",
        "\n",
        "  int true_class = 0;\n",
        "\n",
        "  int* d_count = 0;\n",
        "  gpuErrchk(cudaMalloc((void **)&d_count, sizeof(int)));\n",
        "  gpuErrchk( \n",
        "        cudaMemcpy( d_count, &true_class, sizeof(int), cudaMemcpyHostToDevice )\n",
        "  );\n",
        "\n",
        "    int threadsPerBlock = d_Z.cols;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    evaluate_accuracy_kernel<<< blocksPerGrid, threadsPerBlock >>>(d_Y,d_Z,d_count);\n",
        "    device_synchronize();\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "\n",
        "  gpuErrchk(\n",
        "          cudaMemcpy(&true_class, d_count, sizeof(int), cudaMemcpyDeviceToHost )\n",
        "  );\n",
        "\n",
        "  //printf(\"Correct results: %d out of %d\\n\",true_class,nb_tested);\n",
        "  //printf(\"Accuracy: %f\\n\",(float)true_class/(float)nb_tested);\n",
        "  return (float)true_class/(float)d_Z.cols;\n",
        "}\n",
        "\n",
        "\n",
        "__global__\n",
        "void evaluate_logloss_kernel(fmatrix d_P,fmatrix d_Y, float* dev_a){\n",
        "    /*__shared__ float sum;\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / d_P.rows;\n",
        "    int i = idx % d_P.rows;\n",
        "\n",
        "    if (i==0 && j==0) sum = 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    if (i < d_P.rows && j < d_P.cols ){\n",
        "        sum = sum - getfm(d_Y,i,j) * log10(getfm(d_P,i,j));\n",
        "    }\n",
        "    __syncthreads();\n",
        "    *dev_a = sum;\n",
        "    */\n",
        "\n",
        "    __shared__ float sum;\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / d_P.rows;\n",
        "    int i = idx % d_P.rows;\n",
        "\n",
        "    if (i==0 && j==0){\n",
        "        sum = 0;\n",
        "        for (int k=0; k<d_P.rows; k++){\n",
        "            for (int l=0; l<d_P.cols; l++){\n",
        "                sum = sum - getfm(d_Y,k,l) * log10(getfm(d_P,k,l));\n",
        "            }\n",
        "        }\n",
        "    } \n",
        "    __syncthreads();\n",
        "    *dev_a = sum;\n",
        "}\n",
        "\n",
        "\n",
        "float evaluate_logloss(fmatrix d_P,fmatrix d_Y) {\n",
        "    assert(d_Y.cols == d_P.cols);\n",
        "    assert(d_Y.rows == d_P.rows);\n",
        "\n",
        "  float J = 0.0;\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    // ... compute the logloss here ...\n",
        "    ///////////////////////////////////\n",
        "\n",
        "    int threadsPerBlock = fmatrix_elements(d_P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "     float *dev_a;\n",
        "     float a = 0;\n",
        "     cudaMalloc((void**)&dev_a, sizeof(float));\n",
        "     cudaMemcpy(dev_a, &a, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    evaluate_logloss_kernel<<< blocksPerGrid, threadsPerBlock >>>(d_P, d_Y, dev_a);\n",
        "\n",
        "    cudaMemcpy(&a, dev_a, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(dev_a);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    device_synchronize();\n",
        "\n",
        "  return a;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate_accuracy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AwZh8WULi_F"
      },
      "source": [
        "## Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWO5p1NeHE9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b07e5b-84f9-4b2c-d046-bfb20cf696f9"
      },
      "source": [
        "%%writefile linear_classification.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <fstream>\n",
        "\n",
        "/*Matrix multiplication functions and other auxiliary functions*/\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"read_csv.cuh\"\n",
        "#include \"preprocess_data.cuh\"\n",
        "#include \"classifier_math.cuh\"\n",
        "#include \"evaluate_accuracy.cuh\"\n",
        "/* Includes, cuda */\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "//Number of thread per block\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "/* Constants for housing data set */\n",
        "#define data_columns  (9)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Number of rows in arrays to print for debugging\n",
        "/////////////////////////////////////////////////////////\n",
        "#define print_rows (10)\n",
        "\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "/////////////////////////////////////////////////////////\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    size_t N_train = 5000; //12000; // points for training (Google: 12000)\n",
        "    size_t N_test =3000;// 5000; // points for validation (Google: 5000)\n",
        "    size_t N = N_train;\n",
        "    size_t Nall = N_train+N_test;\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Reading the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    fmatrix alldata = fmatrix_create_on_host(Nall,data_columns);\n",
        "    read_csv(\"sample_data/california_housing_train.csv\",alldata.data,Nall,data_columns);\n",
        "    //fmatrix_print(alldata);\n",
        "\n",
        "    size_t D = data_columns-1+1; // remove output column, add column with const. 1.0\n",
        "    size_t M = 2; // number of labels (one-hot encoding)\n",
        "    fmatrix Xall = fmatrix_create_on_host(D,Nall);\n",
        "    fmatrix Yall = fmatrix_create_on_host(M,Nall);\n",
        "    get_inputs_and_labels(alldata.data,&Xall.data,&Yall.data,Nall,data_columns,D,M);\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Inputs and labels are now available in X and Y.\n",
        "    // Each input is a column in X; X is of dimension D x N\n",
        "    // each label is a column in Y; Y is of dimension M x N\n",
        "    /////////////////////////////////////////////////////////\n",
        "     \n",
        "    // Logfile\n",
        "    FILE* fp = fopen(\"log.txt\", \"w\");\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for Stochastic Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    int nb_iter = 1200;           // defeault: 10;\n",
        "    int periods = nb_iter;      // reporting period\n",
        "    int batch_size = N;         // defeault: N;\n",
        "    float learning_rate = 1e-7; // default: 1e-7\n",
        " \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Memory Allocation and Initialization\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // take X,Y to be the first N columns of all data\n",
        "    fmatrix h_X = fmatrix_subcolumns(Xall,0,N);\n",
        "    fmatrix h_Y = fmatrix_subcolumns(Yall,0,N);\n",
        "    fmatrix h_Xtest = fmatrix_subcolumns(Xall,N,Nall);\n",
        "    fmatrix h_Ytest = fmatrix_subcolumns(Yall,N,Nall);\n",
        "    fmatrix h_W = fmatrix_create_on_host(D,M);\n",
        "    fmatrix h_J = fmatrix_create_on_host(1,1);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Initializing Weight Matrix \n",
        "    // its dimension is D x M\n",
        "    /////////////////////////////////////////////////////////\n",
        "    xavier_weight_init(1.0,h_W);\n",
        " \n",
        "    //////////////////////////////\n",
        "    // Copy data to device      //\n",
        "    //////////////////////////////\n",
        "    fmatrix d_X = fmatrix_copy_to_device(h_X);\n",
        "    fmatrix d_Y = fmatrix_copy_to_device(h_Y);\n",
        "    fmatrix d_Xtest = fmatrix_copy_to_device(h_Xtest);\n",
        "    fmatrix d_Ytest = fmatrix_copy_to_device(h_Ytest);\n",
        "    fmatrix d_W = fmatrix_copy_to_device(h_W);\n",
        "    fmatrix d_J = fmatrix_copy_to_device(h_J);\n",
        " \n",
        "    /////////////////////////////////////////\n",
        "    // Create auxiliary matrices on device //\n",
        "    /////////////////////////////////////////\n",
        "    fmatrix d_Z = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_P = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_G = fmatrix_create_on_device(D,M);\n",
        "    // auxiliary matrix for computing Z=W^T X on test data\n",
        "    fmatrix d_Ztest = fmatrix_create_on_device(M,d_Xtest.cols);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Batch Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    //fmatrix_device_print(d_X);\n",
        "    //fmatrix_device_print(d_W);\n",
        " \n",
        "     /* Evaluate the accuracy */\n",
        "    float accuracy = 0;\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"initial accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    float J = 0;\n",
        " \n",
        "    clock_t tstart_total, tend;\n",
        "    tstart_total = clock();\n",
        " \n",
        "    int batch_pointer = 0;\n",
        "\n",
        "    fmatrix d_X_T = fmatrix_create_on_device(d_X.cols, d_X.rows);\n",
        "    fmatrix_transpose(d_X, d_X_T);\n",
        "    fmatrix mu = fmatrix_create_on_device(d_X_T.rows,1);\n",
        "    fmatrix sigma = fmatrix_create_on_device(d_X_T.rows,1);\n",
        "    fmatrix_compute(d_X_T, mu, sigma);\n",
        "    fmatrix_normalize(d_X_T, mu, sigma);\n",
        "    fmatrix_transpose(d_X_T, d_X);\n",
        "\n",
        " \n",
        "    fmatrix_free_on_device(&mu);\n",
        "    fmatrix_free_on_device(&sigma);\n",
        "    fmatrix_free_on_device(&d_X_T);\n",
        "\n",
        "\n",
        "    for (int i = 0; i < nb_iter; ++i ) {\n",
        "\n",
        "      ////////////////////////////////\n",
        "      // compute Z = W^T X\n",
        "      // --> each column z of Z corresponds to one column x of X\n",
        "      ////////////////////////////////\n",
        "      \n",
        "      ///////////////////////////////////\n",
        "      // TO BE COMPLETED\n",
        "      ///////////////////////////////////\n",
        "      \n",
        "\n",
        "      fmatrix d_W_T = fmatrix_create_on_device(d_W.cols, d_W.rows);\n",
        "      fmatrix_transpose(d_W, d_W_T);\n",
        "      fmatrix_mult(d_Z, 1, d_W_T, d_X);\n",
        "       fmatrix_free_on_device(&d_W_T);\n",
        "\n",
        "      ////////////////////////////////\n",
        "      // For each column z of Z, compute activation p(z);\n",
        "      // then update W\n",
        "      ////////////////////////////////\n",
        "\n",
        "      // compute softmax per column of Z and store in Z\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    ///////////////////////////////////\n",
        "\n",
        "    softmax_col(d_Z, d_Z);\n",
        "    \n",
        "\n",
        "    d_P = d_Z;\n",
        "\n",
        "    \n",
        "\n",
        "      // evaluate logloss (for reporting only)\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    ///////////////////////////////////\n",
        "\n",
        "    J = evaluate_logloss(d_P, d_Y)/N_test;\n",
        "\n",
        "      \n",
        "      // Q:=P-Y\n",
        "      // compute gradient G = XQ^T\n",
        "      // ... possibly work with G here ...\n",
        "      // update weights W = W - learning_rate*G\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    ///////////////////////////////////\n",
        "\n",
        "\n",
        "      fmatrix d_Q =  fmatrix_copy_to_device(d_P);\n",
        "      fmatrix_add(d_Q, -1, d_Y);\n",
        "      \n",
        "\n",
        "      \n",
        "      fmatrix d_Q_T = fmatrix_create_on_device(d_P.cols,d_P.rows);\n",
        "      fmatrix_transpose(d_Q, d_Q_T);\n",
        "\n",
        "      fmatrix_mult(d_G, 1, d_X, d_Q_T);\n",
        "      fmatrix_free_on_device(&d_Q_T);\n",
        "      fmatrix_free_on_device(&d_Q);\n",
        "      \n",
        "      float learning_rate_minus = 0 - learning_rate;\n",
        "      fmatrix_add(d_W, -0.0001, d_G);\n",
        "\n",
        "      //printf(\"W:\\n\");fmatrix_device_print(d_W);\n",
        "      \n",
        "      ////////////////////////////////\n",
        "      // For reporting, compute logloss and accuracy\n",
        "      ////////////////////////////////\n",
        "      if (i%(100)==0) {\n",
        "        float accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "        printf(\"iter: %d, logloss: %f, accuracy: %f\\n\",i,J, accuracy);\n",
        "        fprintf(fp, \"%f,%f\\n\", J, accuracy);\n",
        "      }\n",
        "\n",
        "    }\n",
        "    tend = clock();\n",
        "    float duration = ((float)(tend-tstart_total))/CLOCKS_PER_SEC;\n",
        "    printf(\"Duration (s): %f\\n\",duration);\n",
        "    /* Evaluate the accuracy */\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"final accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    printf(\"final weights: \\n\");\n",
        "    fmatrix_device_print(d_W);\n",
        "\n",
        "    /* Memory clean up */\n",
        "    /** No need to free h_X, h_Y, h_Xtest, h_Ytest since \n",
        "     *  they all point to Xall \n",
        "     */\n",
        "    fmatrix_free_on_host(&h_W);\n",
        "    fmatrix_free_on_host(&Xall);\n",
        "    fmatrix_free_on_host(&Yall);\n",
        "\n",
        "    fmatrix_free_on_device(&d_X);\n",
        "    fmatrix_free_on_device(&d_Y);\n",
        "    fmatrix_free_on_device(&d_Xtest);\n",
        "    fmatrix_free_on_device(&d_Ytest);\n",
        "    fmatrix_free_on_device(&d_W);\n",
        "    fmatrix_free_on_device(&d_Z);\n",
        "    fmatrix_free_on_device(&d_J);\n",
        " \n",
        "    // Close log file\n",
        "    fclose(fp);\n",
        "}"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting linear_classification.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrATC8s9LsDw"
      },
      "source": [
        "# Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z52xd0NMRKXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d028d8-79a1-40f0-cd8a-d1d11730d9f2"
      },
      "source": [
        "!nvcc -arch=sm_35 -Wno-deprecated-gpu-targets -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver linear_classification.cu read_csv.cu preprocess_data.cu evaluate_accuracy.cu fmatrix.cu classifier_math.cu cuda_stuff.cu"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear_classification.cu(74): warning: variable \"periods\" was declared but never referenced\n",
            "\n",
            "linear_classification.cu(130): warning: variable \"batch_pointer\" was declared but never referenced\n",
            "\n",
            "evaluate_accuracy.cu(118): warning: variable \"J\" was declared but never referenced\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVqTfXcLvPr"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GcUSYjJ1EEx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV_vFkIT7fV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8390d27f-64a6-481e-af75-5b800d2492e0"
      },
      "source": [
        "%%time\n",
        "!./a.out \n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 8000 rows.\n",
            "Allocated memory for inputs: 8000 rows, 9 columns.\n",
            "Allocated memory for labels: 8000 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t-114.58\t-114.59\t-114.59\t-114.6\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t33.61\t34.83\t33.61\t34.83\t\n",
            "15\t19\t17\t14\t20\t29\t25\t41\t34\t46\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t2907\t812\t4789\t1497\t\n",
            "1283\t1901\t174\t337\t326\t236\t680\t168\t1175\t309\t\n",
            "1015\t1129\t333\t515\t624\t671\t1841\t375\t3134\t787\t\n",
            "472\t463\t117\t226\t262\t239\t633\t158\t1056\t271\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t2.6768\t1.7083\t2.1782\t2.1908\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "initial accuracy: 0.726333\n",
            "iter: 0, logloss: 0.390491, accuracy: 0.726333\n",
            "iter: 100, logloss: 0.284045, accuracy: 0.738000\n",
            "iter: 200, logloss: 0.276143, accuracy: 0.748667\n",
            "iter: 300, logloss: 0.271930, accuracy: 0.757333\n",
            "iter: 400, logloss: 0.269047, accuracy: 0.764667\n",
            "iter: 500, logloss: 0.266930, accuracy: 0.769667\n",
            "iter: 600, logloss: 0.265309, accuracy: 0.772333\n",
            "iter: 700, logloss: 0.264025, accuracy: 0.776667\n",
            "iter: 800, logloss: 0.262978, accuracy: 0.779667\n",
            "iter: 900, logloss: 0.262102, accuracy: 0.782333\n",
            "iter: 1000, logloss: 0.261353, accuracy: 0.785333\n",
            "iter: 1100, logloss: 0.260698, accuracy: 0.787667\n",
            "Duration (s): 2.396233\n",
            "final accuracy: 0.791000\n",
            "final weights: \n",
            "[\n",
            "1.285218,\t-0.181913;\n",
            "3.139951,\t2.331234;\n",
            "2.228542,\t0.334442;\n",
            "1.958363,\t0.827113;\n",
            "-4.330594,\t-6.359038;\n",
            "-2.717574,\t2.798541;\n",
            "-3.583839,\t-1.037133;\n",
            "0.518223,\t0.617729;\n",
            "2.470902,\t0.781273\n",
            "]\n",
            "CPU times: user 25.7 ms, sys: 10.3 ms, total: 36 ms\n",
            "Wall time: 2.72 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxIyxh0ClrIz"
      },
      "source": [
        "Let's plot the logloss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR2kCNEIlpqQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "211f2248-b1a3-46a5-f4f0-b9abce517160"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "data = pd.read_csv('log.txt',sep=',',header=None)\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(data[0],label=\"logloss\")\n",
        "ax2=ax.twinx()\n",
        "ax2.plot([], [])\n",
        "ax2.plot(data[1],label=\"accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yH3473IN9alJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Mini-Batch "
      ],
      "metadata": {
        "id": "KHSS3L5q9bQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Pk-JL7rAhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af97036-334a-40ca-a748-dace2ba373e0"
      },
      "source": [
        "%%writefile linear_classification.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <fstream>\n",
        "\n",
        "/*Matrix multiplication functions and other auxiliary functions*/\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"read_csv.cuh\"\n",
        "#include \"preprocess_data.cuh\"\n",
        "#include \"classifier_math.cuh\"\n",
        "#include \"evaluate_accuracy.cuh\"\n",
        "/* Includes, cuda */\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "//Number of thread per block\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "/* Constants for housing data set */\n",
        "#define data_columns  (9)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Number of rows in arrays to print for debugging\n",
        "/////////////////////////////////////////////////////////\n",
        "#define print_rows (10)\n",
        "\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "/////////////////////////////////////////////////////////\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    size_t N_train = 12000; //12000; // points for training (Google: 12000)\n",
        "    size_t N_test =5000;// 5000; // points for validation (Google: 5000)\n",
        "    size_t N = N_train;\n",
        "    size_t Nall = N_train+N_test;\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Reading the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    fmatrix alldata = fmatrix_create_on_host(Nall,data_columns);\n",
        "    read_csv(\"sample_data/california_housing_train.csv\",alldata.data,Nall,data_columns);\n",
        "    //fmatrix_print(alldata);\n",
        "\n",
        "    size_t D = data_columns-1+1; // remove output column, add column with const. 1.0\n",
        "    size_t M = 2; // number of labels (one-hot encoding)\n",
        "    fmatrix Xall = fmatrix_create_on_host(D,Nall);\n",
        "    fmatrix Yall = fmatrix_create_on_host(M,Nall);\n",
        "    get_inputs_and_labels(alldata.data,&Xall.data,&Yall.data,Nall,data_columns,D,M);\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Inputs and labels are now available in X and Y.\n",
        "    // Each input is a column in X; X is of dimension D x N\n",
        "    // each label is a column in Y; Y is of dimension M x N\n",
        "    /////////////////////////////////////////////////////////\n",
        "     \n",
        "    // Logfile\n",
        "    FILE* fp = fopen(\"log.txt\", \"w\");\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for Stochastic Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    int nb_iter = 100;           // defeault: 10;\n",
        "    int periods = nb_iter;      // reporting period\n",
        "    int batch_size = 100;         // defeault: N;\n",
        "    float learning_rate = 0.005; // default: 1e-7\n",
        " \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Memory Allocation and Initialization\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // take X,Y to be the first N columns of all data\n",
        "    fmatrix h_X = fmatrix_subcolumns(Xall,0,N);\n",
        "    fmatrix h_Y = fmatrix_subcolumns(Yall,0,N);\n",
        "    fmatrix h_Xtest = fmatrix_subcolumns(Xall,N,Nall);\n",
        "    fmatrix h_Ytest = fmatrix_subcolumns(Yall,N,Nall);\n",
        "    fmatrix h_W = fmatrix_create_on_host(D,M);\n",
        "    fmatrix h_J = fmatrix_create_on_host(1,1);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Initializing Weight Matrix \n",
        "    // its dimension is D x M\n",
        "    /////////////////////////////////////////////////////////\n",
        "    xavier_weight_init(1.0,h_W);\n",
        " \n",
        "    //////////////////////////////\n",
        "    // Copy data to device      //\n",
        "    //////////////////////////////\n",
        "    fmatrix d_X = fmatrix_copy_to_device(h_X);\n",
        "    fmatrix d_Y = fmatrix_copy_to_device(h_Y);\n",
        "    fmatrix d_Xtest = fmatrix_copy_to_device(h_Xtest);\n",
        "    fmatrix d_Ytest = fmatrix_copy_to_device(h_Ytest);\n",
        "    fmatrix d_W = fmatrix_copy_to_device(h_W);\n",
        "    fmatrix d_J = fmatrix_copy_to_device(h_J);\n",
        " \n",
        "    /////////////////////////////////////////\n",
        "    // Create auxiliary matrices on device //\n",
        "    /////////////////////////////////////////\n",
        "    fmatrix d_Z = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_P = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_G = fmatrix_create_on_device(D,M);\n",
        "    // auxiliary matrix for computing Z=W^T X on test data\n",
        "    fmatrix d_Ztest = fmatrix_create_on_device(M,d_Xtest.cols);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Batch Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    //fmatrix_device_print(d_X);\n",
        "    //fmatrix_device_print(d_W);\n",
        " \n",
        "     /* Evaluate the accuracy */\n",
        "    float accuracy = 0;\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"initial accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    float J = 0;\n",
        " \n",
        "    clock_t tstart_total, tend;\n",
        "    tstart_total = clock();\n",
        " \n",
        "    int batch_pointer = 0;\n",
        "\n",
        "\n",
        "    fmatrix d_X_1[N_train/batch_size];\n",
        "    fmatrix d_Y_1[N_train/batch_size];\n",
        "    for (int i = 0; i<N_train/batch_size; i++){\n",
        "      d_X_1[i] = fmatrix_subcolumns(d_X,i*(batch_size),(i+1)*(batch_size));\n",
        "      d_Y_1[i] = fmatrix_subcolumns(d_Y,i*(batch_size),(i+1)*(batch_size));\n",
        "      fmatrix d_X_T_1 = fmatrix_create_on_device(d_X_1[i].cols, d_X_1[i].rows);\n",
        "      fmatrix_transpose(d_X_1[i], d_X_T_1);\n",
        "      fmatrix mu = fmatrix_create_on_device(d_X_T_1.rows,1);\n",
        "      fmatrix sigma = fmatrix_create_on_device(d_X_T_1.rows,1);\n",
        "      fmatrix_compute(d_X_T_1, mu, sigma);\n",
        "      fmatrix_normalize(d_X_T_1, mu, sigma);\n",
        "      fmatrix_transpose(d_X_T_1, d_X_1[i]);\n",
        "      fmatrix_free_on_device(&mu);\n",
        "      fmatrix_free_on_device(&sigma);\n",
        "      fmatrix_free_on_device(&d_X_T_1);\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < nb_iter; ++i ) {\n",
        "        for(int k = 0; k < N_train/batch_size; ++k){\n",
        "            \n",
        "      ////////////////////////////////\n",
        "      // compute Z = W^T X\n",
        "      // --> each column z of Z corresponds to one column x of X\n",
        "      ////////////////////////////////\n",
        "      \n",
        "      ///////////////////////////////////\n",
        "      // TO BE COMPLETED\n",
        "      ///////////////////////////////////\n",
        "\n",
        "          fmatrix d_W_T = fmatrix_create_on_device(d_W.cols, d_W.rows);\n",
        "          fmatrix_transpose(d_W, d_W_T);\n",
        "          fmatrix_mult(d_Z, 1, d_W_T, d_X_1[k]);\n",
        "          fmatrix_free_on_device(&d_W_T);\n",
        "\n",
        "      ////////////////////////////////\n",
        "      // For each column z of Z, compute activation p(z);\n",
        "      // then update W\n",
        "      ////////////////////////////////\n",
        "\n",
        "      // compute softmax per column of Z and store in Z\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    ///////////////////////////////////\n",
        "\n",
        "          softmax_col(d_Z, d_Z);\n",
        "          d_P = d_Z;\n",
        "\n",
        "      // evaluate logloss (for reporting only)\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    ///////////////////////////////////\n",
        "\n",
        "          if (k == 0) J = 0;\n",
        "          else J += evaluate_logloss(d_P, d_Y_1[k]);\n",
        "          if (k == N_train/batch_size - 1) J /= N_train;\n",
        "          \n",
        "\n",
        "\n",
        "      // Q:=P-Y\n",
        "      // compute gradient G = XQ^T\n",
        "      // ... possibly work with G here ...\n",
        "      // update weights W = W - learning_rate*G\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    ///////////////////////////////////\n",
        "\n",
        "\n",
        "          fmatrix d_Q =  fmatrix_copy_to_device(d_P);\n",
        "          fmatrix_add(d_Q, -1, d_Y_1[k]);\n",
        "      \n",
        "          fmatrix d_Q_T = fmatrix_create_on_device(d_P.cols,d_P.rows);\n",
        "          fmatrix_transpose(d_Q, d_Q_T);\n",
        "\n",
        "          fmatrix_mult(d_G, 1, d_X_1[k], d_Q_T);\n",
        "          fmatrix_free_on_device(&d_Q_T);\n",
        "          fmatrix_free_on_device(&d_Q);\n",
        "      \n",
        "          float learning_rate_minus = 0 - learning_rate;\n",
        "          fmatrix_add(d_W, learning_rate_minus, d_G);\n",
        "\n",
        "      //printf(\"W:\\n\");fmatrix_device_print(d_W);\n",
        "      \n",
        "      ////////////////////////////////\n",
        "      // For reporting, compute logloss and accuracy\n",
        "      ////////////////////////////////\n",
        "          if (k == N_train/batch_size - 1) {\n",
        "            float accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "            printf(\"epoch: %d, logloss: %f, accuracy: %f\\n\",i,J, accuracy);\n",
        "            fprintf(fp, \"%f,%f\\n\", J, accuracy);\n",
        "          }\n",
        "        }\n",
        "    }\n",
        "    tend = clock();\n",
        "    float duration = ((float)(tend-tstart_total))/CLOCKS_PER_SEC;\n",
        "    printf(\"Duration (s): %f\\n\",duration);\n",
        "    /* Evaluate the accuracy */\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"final accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    printf(\"final weights: \\n\");\n",
        "    fmatrix_device_print(d_W);\n",
        "\n",
        "    /* Memory clean up */\n",
        "    /** No need to free h_X, h_Y, h_Xtest, h_Ytest since \n",
        "     *  they all point to Xall \n",
        "     */\n",
        "    fmatrix_free_on_host(&h_W);\n",
        "    fmatrix_free_on_host(&Xall);\n",
        "    fmatrix_free_on_host(&Yall);\n",
        "\n",
        "    fmatrix_free_on_device(&d_X);\n",
        "    fmatrix_free_on_device(&d_Y);\n",
        "    fmatrix_free_on_device(&d_Xtest);\n",
        "    fmatrix_free_on_device(&d_Ytest);\n",
        "    fmatrix_free_on_device(&d_W);\n",
        "    fmatrix_free_on_device(&d_Z);\n",
        "    fmatrix_free_on_device(&d_J);\n",
        " \n",
        "    // Close log file\n",
        "    fclose(fp);\n",
        "}"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting linear_classification.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_35 -Wno-deprecated-gpu-targets -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver linear_classification.cu read_csv.cu preprocess_data.cu evaluate_accuracy.cu fmatrix.cu classifier_math.cu cuda_stuff.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnp0WuNbo21E",
        "outputId": "1659c8df-a87e-4ee0-ffe0-d6032708d214"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear_classification.cu(74): warning: variable \"periods\" was declared but never referenced\n",
            "\n",
            "linear_classification.cu(130): warning: variable \"batch_pointer\" was declared but never referenced\n",
            "\n",
            "evaluate_accuracy.cu(118): warning: variable \"J\" was declared but never referenced\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!./a.out "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQi31tAMo8tN",
        "outputId": "8f587542-3ee2-4283-d9fe-e66302acb4dd"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 17000 rows.\n",
            "Allocated memory for inputs: 17000 rows, 9 columns.\n",
            "Allocated memory for labels: 17000 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t-114.58\t-114.59\t-114.59\t-114.6\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t33.61\t34.83\t33.61\t34.83\t\n",
            "15\t19\t17\t14\t20\t29\t25\t41\t34\t46\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t2907\t812\t4789\t1497\t\n",
            "1283\t1901\t174\t337\t326\t236\t680\t168\t1175\t309\t\n",
            "1015\t1129\t333\t515\t624\t671\t1841\t375\t3134\t787\t\n",
            "472\t463\t117\t226\t262\t239\t633\t158\t1056\t271\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t2.6768\t1.7083\t2.1782\t2.1908\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "initial accuracy: 0.660200\n",
            "epoch: 0, logloss: 0.204232, accuracy: 0.660000\n",
            "epoch: 1, logloss: 0.203022, accuracy: 0.367000\n",
            "epoch: 2, logloss: 0.202438, accuracy: 0.392800\n",
            "epoch: 3, logloss: 0.201989, accuracy: 0.421200\n",
            "epoch: 4, logloss: 0.201606, accuracy: 0.453600\n",
            "epoch: 5, logloss: 0.201282, accuracy: 0.482400\n",
            "epoch: 6, logloss: 0.201004, accuracy: 0.506000\n",
            "epoch: 7, logloss: 0.200763, accuracy: 0.525600\n",
            "epoch: 8, logloss: 0.200551, accuracy: 0.541000\n",
            "epoch: 9, logloss: 0.200363, accuracy: 0.561800\n",
            "epoch: 10, logloss: 0.200192, accuracy: 0.582800\n",
            "epoch: 11, logloss: 0.200037, accuracy: 0.600000\n",
            "epoch: 12, logloss: 0.199894, accuracy: 0.613000\n",
            "epoch: 13, logloss: 0.199762, accuracy: 0.629200\n",
            "epoch: 14, logloss: 0.199638, accuracy: 0.644000\n",
            "epoch: 15, logloss: 0.199522, accuracy: 0.653400\n",
            "epoch: 16, logloss: 0.199412, accuracy: 0.661600\n",
            "epoch: 17, logloss: 0.199308, accuracy: 0.670200\n",
            "epoch: 18, logloss: 0.199210, accuracy: 0.672200\n",
            "epoch: 19, logloss: 0.199115, accuracy: 0.678400\n",
            "epoch: 20, logloss: 0.199025, accuracy: 0.682600\n",
            "epoch: 21, logloss: 0.198938, accuracy: 0.688600\n",
            "epoch: 22, logloss: 0.198854, accuracy: 0.694800\n",
            "epoch: 23, logloss: 0.198774, accuracy: 0.696200\n",
            "epoch: 24, logloss: 0.198696, accuracy: 0.699800\n",
            "epoch: 25, logloss: 0.198621, accuracy: 0.704200\n",
            "epoch: 26, logloss: 0.198547, accuracy: 0.707600\n",
            "epoch: 27, logloss: 0.198477, accuracy: 0.711600\n",
            "epoch: 28, logloss: 0.198408, accuracy: 0.714400\n",
            "epoch: 29, logloss: 0.198340, accuracy: 0.717600\n",
            "epoch: 30, logloss: 0.198275, accuracy: 0.719200\n",
            "epoch: 31, logloss: 0.198211, accuracy: 0.722400\n",
            "epoch: 32, logloss: 0.198149, accuracy: 0.723200\n",
            "epoch: 33, logloss: 0.198087, accuracy: 0.725600\n",
            "epoch: 34, logloss: 0.198028, accuracy: 0.729000\n",
            "epoch: 35, logloss: 0.197969, accuracy: 0.729400\n",
            "epoch: 36, logloss: 0.197912, accuracy: 0.730600\n",
            "epoch: 37, logloss: 0.197856, accuracy: 0.731600\n",
            "epoch: 38, logloss: 0.197800, accuracy: 0.733400\n",
            "epoch: 39, logloss: 0.197746, accuracy: 0.735600\n",
            "epoch: 40, logloss: 0.197693, accuracy: 0.736600\n",
            "epoch: 41, logloss: 0.197640, accuracy: 0.737200\n",
            "epoch: 42, logloss: 0.197589, accuracy: 0.736400\n",
            "epoch: 43, logloss: 0.197538, accuracy: 0.736800\n",
            "epoch: 44, logloss: 0.197488, accuracy: 0.736800\n",
            "epoch: 45, logloss: 0.197439, accuracy: 0.736000\n",
            "epoch: 46, logloss: 0.197390, accuracy: 0.735600\n",
            "epoch: 47, logloss: 0.197342, accuracy: 0.735000\n",
            "epoch: 48, logloss: 0.197295, accuracy: 0.736200\n",
            "epoch: 49, logloss: 0.197248, accuracy: 0.736000\n",
            "epoch: 50, logloss: 0.197202, accuracy: 0.736200\n",
            "epoch: 51, logloss: 0.197156, accuracy: 0.736600\n",
            "epoch: 52, logloss: 0.197111, accuracy: 0.737000\n",
            "epoch: 53, logloss: 0.197067, accuracy: 0.737800\n",
            "epoch: 54, logloss: 0.197023, accuracy: 0.737200\n",
            "epoch: 55, logloss: 0.196979, accuracy: 0.737200\n",
            "epoch: 56, logloss: 0.196936, accuracy: 0.737000\n",
            "epoch: 57, logloss: 0.196894, accuracy: 0.737600\n",
            "epoch: 58, logloss: 0.196851, accuracy: 0.737600\n",
            "epoch: 59, logloss: 0.196810, accuracy: 0.737800\n",
            "epoch: 60, logloss: 0.196768, accuracy: 0.738200\n",
            "epoch: 61, logloss: 0.196727, accuracy: 0.738000\n",
            "epoch: 62, logloss: 0.196687, accuracy: 0.738200\n",
            "epoch: 63, logloss: 0.196646, accuracy: 0.737400\n",
            "epoch: 64, logloss: 0.196607, accuracy: 0.736600\n",
            "epoch: 65, logloss: 0.196567, accuracy: 0.736800\n",
            "epoch: 66, logloss: 0.196528, accuracy: 0.736600\n",
            "epoch: 67, logloss: 0.196489, accuracy: 0.736800\n",
            "epoch: 68, logloss: 0.196450, accuracy: 0.736600\n",
            "epoch: 69, logloss: 0.196412, accuracy: 0.736600\n",
            "epoch: 70, logloss: 0.196374, accuracy: 0.736600\n",
            "epoch: 71, logloss: 0.196336, accuracy: 0.737400\n",
            "epoch: 72, logloss: 0.196299, accuracy: 0.737600\n",
            "epoch: 73, logloss: 0.196262, accuracy: 0.737800\n",
            "epoch: 74, logloss: 0.196225, accuracy: 0.738000\n",
            "epoch: 75, logloss: 0.196188, accuracy: 0.738800\n",
            "epoch: 76, logloss: 0.196152, accuracy: 0.738600\n",
            "epoch: 77, logloss: 0.196116, accuracy: 0.739200\n",
            "epoch: 78, logloss: 0.196080, accuracy: 0.739200\n",
            "epoch: 79, logloss: 0.196044, accuracy: 0.739800\n",
            "epoch: 80, logloss: 0.196009, accuracy: 0.739200\n",
            "epoch: 81, logloss: 0.195973, accuracy: 0.739400\n",
            "epoch: 82, logloss: 0.195938, accuracy: 0.739800\n",
            "epoch: 83, logloss: 0.195903, accuracy: 0.740000\n",
            "epoch: 84, logloss: 0.195869, accuracy: 0.740000\n",
            "epoch: 85, logloss: 0.195834, accuracy: 0.740000\n",
            "epoch: 86, logloss: 0.195800, accuracy: 0.740000\n",
            "epoch: 87, logloss: 0.195766, accuracy: 0.740200\n",
            "epoch: 88, logloss: 0.195732, accuracy: 0.739800\n",
            "epoch: 89, logloss: 0.195699, accuracy: 0.740200\n",
            "epoch: 90, logloss: 0.195665, accuracy: 0.740600\n",
            "epoch: 91, logloss: 0.195632, accuracy: 0.740200\n",
            "epoch: 92, logloss: 0.195599, accuracy: 0.739800\n",
            "epoch: 93, logloss: 0.195566, accuracy: 0.739600\n",
            "epoch: 94, logloss: 0.195533, accuracy: 0.739000\n",
            "epoch: 95, logloss: 0.195500, accuracy: 0.739000\n",
            "epoch: 96, logloss: 0.195468, accuracy: 0.739200\n",
            "epoch: 97, logloss: 0.195435, accuracy: 0.738600\n",
            "epoch: 98, logloss: 0.195403, accuracy: 0.738400\n",
            "epoch: 99, logloss: 0.195371, accuracy: 0.738200\n",
            "Duration (s): 1.335952\n",
            "final accuracy: 0.738200\n",
            "final weights: \n",
            "[\n",
            "1.340009,\t-1.198565;\n",
            "2.417130,\t-2.066447;\n",
            "1.054958,\t-0.989109;\n",
            "0.803290,\t-0.740907;\n",
            "2.225388,\t-2.326854;\n",
            "-3.515254,\t3.579338;\n",
            "-3.980151,\t3.906696;\n",
            "0.508235,\t0.015446;\n",
            "1.298291,\t-1.085414\n",
            "]\n",
            "CPU times: user 26.4 ms, sys: 8.97 ms, total: 35.4 ms\n",
            "Wall time: 1.72 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "data = pd.read_csv('log.txt',sep=',',header=None)\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(data[0],label=\"logloss\")\n",
        "ax2=ax.twinx()\n",
        "ax2.plot([], [])\n",
        "ax2.plot(data[1],label=\"accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "hDgetznIqx_3",
        "outputId": "351b2b57-96e3-481b-fca0-9fd2dd29e736"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAD4CAYAAADcpoD8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9bn48c8zk5WQPQFCAiRA2EGWgAurO+5araLVatX662LVettebXvrrbb3utxrta21pVav1roVreKKiKAIogRB9jVsIWENeyDr8/vjnMAYs0ySSSbJed6v17xm5qzfkwPnme8uqooxxhjTVnzhToAxxhhvscBjjDGmTVngMcYY06Ys8BhjjGlTFniMMca0qYhwJ6ApfD6fxsbGhjsZxhjToZSWlqqqtpuMRocKPLGxsRw9ejTcyTDGmA5FRI6FOw2B2k0ENMYYEx4iMlVE1onIRhG5p471vxORZe5rvYgcCFhXFbBuZlDn60gdSOPi4tRyPMYY0zQiUqqqcfWs8wPrgXOBQmAxcK2qrq5n+x8Bo1T1Zvf7EVXt2pT0WI7HGGO8bRywUVULVLUceAm4rIHtrwVebMkJLfAYY0znFyEi+QGv2wLWZQLbA74Xusu+RkT6ADnAhwGLY9xjLhKRy4NKTBMTb4wxpuOpVNW8EBxnGjBDVasClvVR1R0i0hf4UERWqOqmhg5iOR5jjPG2HUCvgO9Z7rK6TKNWMZuq7nDfC4B5wKjGTmiBxxhjvG0xkCsiOSIShRNcvtY6TUQGAcnApwHLkkUk2v2cBowH6myUEMgTRW2vL93BkbJKrj+tT7iTYoxpifKjIH6IjKl7vSpUlELZYag4BpXHoaqi8eNqNVSWQeUxZ/voeIhJgpgE55zHDsCx/XC85v0gREQ728Qmuy/3c1QcRMQ660VCe/2tQFUrReR2YBbgB55W1VUicj+Qr6o1QWga8JJ+tSn0YOAvIlKNk5F5sL7WcIE80Zz6tufy2bLvKO//eHIrpMqYJqj5/9bSB5Kq81CteSCWHQZ/xMkHXlW5s76yHOK7Q0KWsz4cVJ301ASC8qOwfwvs3wwHtjt/i4hYiIiC0hI4vBMOFwcEDHWu8fBOKDvkLIpNhvieTgCqOO4EjLIjznbVQQSathKTBAk9Ib4HRCecvO/VlW66jzvL4jOcV1y6c0019zEyFiJinPeMkRDVpVnJaKg5dTh4IsfTMymWTzftC3cyTGuqrnIeZrvXOO+Hi52XKiRnQ0qO87A6dsD51Voe8APGHwVJvZ1tkrKdX64+f/3nKdkMqPNAiIhxHnSVx50HSdkh5+H3ldcBOLQDSgqctGk1dO0BCRlffRiJ7+RDprrKvYadcKzk5Pm1+uQDiyb8aPRFuNfYF5JznGv1R7nBoMx9+Mc4Dz3Vk7/+FfdBGAP+SKCOgFl+1AkiJZvhUJGzX00waCytvkj37+oGC3+085CO7+Gcs0a3wdDvLOjaHbTK+bscKnaOHe8+nKPiTuY8ohNOPrTrS3cgEedBHxHr/F1q7mPZIYjq6hw3MHcTk+D8jWrucU1O6Nh+KC89ee3H9jv38VARHCwMOJ//ZICproStnzrbNRQ0f7gY0gc0fB0dRFCBR0SmAo/jZMOeUtUHa62/G7gVqAT2ADer6lZ33Y3AL91Nf6Oqz9badybQV1WHteRCGpKRGMPhskoOH68gPiaytU5j2lr5UVj9Bix7AQoXuw84V0Ss82AHWDPT+c8dNHEeLLEp7kMwA2ISYc9aKP7SKcpp0rESnV+9Kf2g/zlOEDi8Ew4XQenek5tWVzoPs4rjznMyPgO6D4UuKU5QqjlezQMrMtZ5yMa4D9qaAFhZ5jxsI2Kd98PFTlAoKXACxPbFUHawCdcQhOgEJ8AnZzu/ymse4jVBqyag1gSIxCwnAMZngM/nBNrK4xDZpUMUTwHO3za6KyT1anzbYKg6ge4rQdv9YVBxzPmbdRKNBh63V+sTBPRqFZGZtcrxlgJ5qloqIt8HHgauEZEU4D4gD+cnzxJ33/3usb8BHAnpFdUhI8kZWLT44HELPB1deSls+hDWvu0ElPIjzq/4vJudh3S3wc4DPibx5AOsqhIOFTo5j5pfrFFdT66vKHVyIiWb4cC2k79gS/fB4V1OsCndB2kDYPS3occI51dxzcOhpogrMsY5b0zSyfL+6ETnwdqeqFt0VV11MjConryemtxPRIzzubG6kogYNzi2IGD4/E5A8jJxf6TEJIY7Ja0umBzPiV6tACJS06v1ROBR1bkB2y8Crnc/nw/MVtUSd9/ZwFTgRRHpCtwN3Aa80sLraFDPRCfLXnTgGAO6x7fmqUwoVVU4RWe7VzuvnSth6wLnIRiTCEMuh1Hfgt6nN/zQ80e4v8brWR8V5wSt7kNb4yraHxEnUNQWEVX3Q88fCSS0erKMdwQTeOrq1XpqA9vfArzbwL41PWIfAP4XaLDcwu1hextAVFRUEMn9usAcj2mHKsth3dtOmf3xA3B0D+xc4bxqis98kSdzHIMugj7j3QeiMaajCWnjAhG5HqdYrcHmYyIyEuinqj8WkeyGtlXV6cB0cFq1NSdd3eOj8YmT4zHtzIYP4L17YN8Gd4Fb3NBtCIy9FXqOgh7DneI0CzTGdArBBJ6gerWKyDnAL4DJqloWsO+UWvvOA04H8kRki5uGbiIyT1UDtw2ZCL+P7gkxFB2wHE+7oOo0Bpj/KKx/16mTmfYi9DndqaSur0WZMaZTCCbwnOjVihNIpgHXBW4gIqOAvwBTVXV3wKpZwH+JSE3p+nnAvW6dz5PuvtnAW60VdGpkJMZQfNByPGFTdsRpUVW4GPKfdorRouLhnF/Dad93WkEZYzyh0cATZK/WR4CuwD/FqeTdpqqXqmqJiDyAE7wA7q9paNDWMpJiWV10KByn9q7jh2DBY7D0eTiy6+Ty7sPg4t/B8G86PcSNMZ7iiZELAH779mqe+3Qrax+YinSUfgIdVcUxJ9jMe9DppzLoYsgc7dTTpA1w6m/sHhjTZmzkgjDJSIylrLKakqPlpHa1Yp2QO7wTVvwTNs6BrQuhqgyyJ8K59ztBxxhjXJ4JPD2TnL48xQePW+AJpbLDsOBxWPhHpwNi+iCnNdrAqU7gsZyNMaYWzwSejESnL0/RgWMMy+z8PYNbXXkpfPEczP8fp9/NsCvhzF9Aar9wp8wY0855JvD0tE6koVF2GBY/5eRwSvdCnwlw7cuQNSbcKTPGdBCeCTypcVFE+X0UWZPq5ts0F974oTPScr+zYdJPoM8Z4U6VMaaD8Uzg8fmEHokxFFsn0qYrL4UP/hM+/wuk5sLN70PvhkZNMsaY+nkm8IDTidSGzWkCVVj3Lsy61xm9+dTvwzn3OcPbG2NMM3kq8PRMiuXzzWHpv9rx7N0A7/47bJoDaQPhxjchZ1K4U2WM6QTa2UQhrSsjMYadh45TVd1xOs2GxdLn4c8ToTAfpj4I319gQceYTkxEporIOhHZKCL31LH+dyKyzH2tF5EDAetuFJEN7uvGYM7nqRxPRlIsVdXKnsNl9EiMaXwHryk/Cm//BL58wQk033gK4ruHO1XGmFYUzGSfqvrjgO1/BIxyPzc42Wd9PJXjyXQ7kVrLtjpsXQjTp8CXL8Lkf4cbXregY4w3nJjsU1XLgZrJPutzLfCi+/nEZJ9usKmZ7LNB3srxuJ1Iiw8ch95hTkx7UVoCs//DKV5L7A03vAb9zgp3qowxoRUhIvkB36e7c51BEyb7FJE+QA7wYQP7Ztbe72uJCTLRnULPgNELPK1wCWyc7UwjvX0xVFfA+DudnI7X5703pnOqVNW8EBxnGjBDVatachBPBZ6E2Ai6RPm9XdS29HmnEygCPYY5U0mPuRG6Dw13yowx4RHUZJ+uacAPa+07pda+8xo7oacCj4g4E8J5tRPp6jdg5o+corSrnoHYpHCnyBgTfo1O9gkgIoOAZODTgMV1TvbZ2Ak9FXjA6cvjyZlIN34AM26BrLFwzfNWpGaMAYKe7BOcgPSSBkzi1tzJPj0zEVyNX76+gteXFvHlfefh93lkyP6tC+H5K52Ro298y3I6xnhMe5sIzlPNqQHG9EnmSFkla3d6ZBrs7YvhH9+ExCy4/jULOsaYsPNc4BmbnQJA/pYG+zd1DkXLnJxOXDp8eyZ07RbuFBljjPcCT2ZSLBmJMXy+pZOP2VbwEfz9cohJdMZZS8gId4qMMQbwYOAREcZmp5C/pYSOVL8VtKoKmH0fPHeZk9O58Q1I6tX4fsYY00Y8F3gAxmYns+tQGYX7O1nrtsM74W/nwYLHnL45t30EKX3DnSpjjPkKzzWnBshz63k+31xCr5QuYU5NiJSWwHOXw8HtcPVzMKShoZaMMSZ8PJnjGdg9nviYCPK3dpJ6nrLDTiOCkgK49kULOsaYds2TgcfnE/L6JHeOSeGO7oMXr4XiL+HqZ23eHGNMu+fJojaAsTkpzF23h31HykjtGh3u5DRNVQWsmAErZ8CmuaDV8I3pMPCCcKfMGGMa5d3AU9OfZ+t+zh/aI8ypaaK3/w2+eNaZxuCMH8GIq22QT2NMh+HZwDMiK5GoCB/5W0o6VuDZMNsJOqffDuf9BsQjw/4YYzoNT9bxAERH+DklK5HFHWkEg2P7ndGl0wfD2b+yoGOM6ZA8G3gATs1JZcWOgxwoLQ93UoLz7j1wZDdc8SREdLB6KWOMcXk68Jw9uBtV1cq8dXvCnZTGrXkLlr8EE/8Neo4Kd2qMMabZvBF43rgdXvrW1xafkpVEenw0s1fvCkOimuDwLnjzDugxAib9NNypMcaYFvFG4Dm6Bw5s+9pin084Z3A3Plq/h7LKFk0h3nqqq+H170P5UbjyKYiICneKjDGmRbwReHwRUF1Z56pzBnfnSFkliwraaWfSz6fDpjlw/m8hfWC4U2OMMS3mjcDjj3Q6XdZhfP80YiP9fNAei9t2rYbZv4IBUyHvlnCnxhjTSYnIVBFZJyIbReSeera5WkRWi8gqEXkhYHmViCxzXzPr2rc2bwQeXyRU1x14YiL9TBqQxgdrdrWvaRKqq+CNH0J0PFz6R2s6bYxpFSLiB54ALgCGANeKyJBa2+QC9wLjVXUocFfA6mOqOtJ9XRrMOYMKPI1FQxG5242Ey0Vkjoj0CVh3o4hscF83usu6iMjbIrLWjZ4PBpOOZvNHQlXdRW3gFLcVHzzOqqJ2NB12/tNQ9AVc8BB0TQ93aowxndc4YKOqFqhqOfASUHuk4e8CT6jqfgBV3d2SEzYaeIKJhsBSIE9VRwAzgIfdfVOA+4BTcS7uPhFJdvf5H1UdBIwCxotI6w005ouoN8cDcNagbvgE3m8vxW2Hd8Kc+6HvFBh2ZbhTY4zp+CJEJD/gdVvAukxge8D3QndZoAHAABFZICKLRGRqwLoY95iLROTyYBITTI6n0WioqnNVtdT9ugjIcj+fD8xW1RI3Us4GpqpqqarOdfctB74I2Cf0GqjjAUjtGs2YPsntp55n1s+hsgwuetSK2IwxoVCpqnkBr+lN3D8CyAWmANcCfxWRJHddH1XNA64DHhORfo0dLJjAE0w0DHQL8G6w+7qJvwSYU9fBROS2mihdWVl/cVmDfA0HHoDzhvRgdfEhNu892rxzhMrGObDyVaejaGqj988YY1pqB9Ar4HuWuyxQITBTVStUdTOwHicQoao73PcCYB5OKVaDQtq4QESuB/KAR4LcPgJ4Efi9m+ivUdXpNVE6IqKZY5r6Gy5qA7jklJ6IwOtLa/+929jc/4LkHJhwV+PbGmNMyy0GckUkR0SigGlA7dZpr+PkdhCRNJyitwIRSRaR6IDl44HVjZ0wmMATTDRERM4BfgFcqqplQe47Hdigqo8FkY7m80c1muPpkRjDGf1SeX3ZjvC1btu1Cnbkw7jbbCw2Y0ybUNVK4HZgFrAGeEVVV4nI/SJS00ptFrBPRFYDc4Gfquo+YDCQLyJfussfVNVGA08wWYgT0RAnaEzDKcs7QURGAX/Bqb8JbO0wC/ivgAYF5+E0yUNEfgMkArcGkYaW8UWCVoFqg3Uml4/M5KczlrN0+wFG906ud7tW88VzTpAccU3bn9sY41mq+g7wTq1lvwr4rMDd7itwm4XA8Kaer9EcT5DR8BGgK/DPwE5EqloCPIATvBYD96tqiYhk4eSOhgBfuPu0XgDyu/G1kVzP1GE9iI7whae4reI4fPkSDL4E4lLb/vzGGNNGgqo0CSIantPAvk8DT9daVgi0XXMtX6TzXl0B1D/WWXxMJOcO6c6bXxbxHxcPIdLfhv1r17wJxw/A6G+33TmNMSYMvDFygd8NPI3keACuGJXJ/tIKPmrrqRK+eBaSsyF7Utue1xhj2pg3As+JHE/jzbEnDUgnuUsk/1rWhsVt+zbBlvkw6gbweeOWGGO8yxtPuSDreAAi/T4uOaUnH6zexcFjjW8fEkv/DuKHkV+fM8gYYzobbwSer9TxNO7qvF6UVVbzz/ztjW/cUtVVTqOC3HMhIaP1z2eMMWHmjcDThDoegGGZieT1Sebvi7ZSXd3KfXq2zIfDxXDKtNY9jzHGtBPeCDw+t6gtiDqeGjeekc3WfaXMW9+iQVgbt/yfEBXvzLljjDEe4I3A08QcDzh9eronRPN/C7e2UqKAimOw+g0YchlExrbeeYwxph3xRuCpqeOpKg96l0i/j2+d2oeP1+9h054jrZOude9C+WEYcXXrHN8YY9ohbwQef/DNqQNdO643UX4ff/+0lXI9y1+B+J6QPaF1jm+MMe2QtwJPE4raANLjo7loRAYzlhRy+HiIm1Yf3QcbZ8PwK8HnD+2xjTGmHfNG4Glic+pAN4/P4UhZJc+FOtez6jUnB2YDghpjPMYbgedEjqfpE8kNz0rkzIHp/HV+AUfKmjkRXV2+fBG6DYHuw0J3TGOM6QC8EXhONKduXnHZnecM4EBpRejqerZ/DjuWwOgbbWprY4zneCPwNLOOp8bIXklMHuDkeo6GItfz6R8hJhFGXd/yYxljTAfjjcDTgjqeGnecnUvJ0XKeX9TCXM/+Lc4UCGO+A9FdW3YsY4zpgLwReFpQx1NjTJ9kJuamMf3jAkrLW5DrWfRnEJ8zvbUxxrQDIjJVRNaJyEYRuaeeba4WkdUiskpEXghYfqOIbHBfNwZzPm8EnhbW8dS465xc9h0t568fb27eAY4dcEaiHnYlJGa2KC3GGBMKIuIHngAuwJkV+loRGVJrm1zgXmC8qg4F7nKXpwD3AacC44D7RCS5sXN6I/C0sI6nxpg+KVw0PIM/f7SJ4oPHmn6AL56F8iNw2g9alA5jjAmhccBGVS1Q1XLgJeCyWtt8F3hCVfcDqGrNIJbnA7NVtcRdNxtodOBJbwSeJkwE15h7LhhElSoPv7euaTtWV8PnT0H2ROg5ssXpMMaYEMkEAueAKXSXBRoADBCRBSKySESmNmHfr/FG4DkxEVzwY7XVp1dKF26dkMO/lu5g6bb9we+4Ix8ObrOWbMaYcIgQkfyAV1MrmSOAXGAKcC3wVxFJam5ivBF4fKEpaqvxgzP7kx4fzf1vrUY1yPl6Vr4G/mgYeGFI0mCMMU1Qqap5Aa/pAet2AL0Cvme5ywIVAjNVtUJVNwPrcQJRMPt+jTcCjz/KeW9h44IaXaMj+On5A1m67QAzlhQ2vkN1Nax+HfqfAzEJIUmDMcaEyGIgV0RyRCQKmAbMrLXN6zi5HUQkDaforQCYBZwnIsluo4Lz3GUN8kjgaXlz6tquGp3F2OxkfvP2GnYfPt7wxtsXObOMDvtGyM5vjDGhoKqVwO04AWMN8IqqrhKR+0XkUnezWcA+EVkNzAV+qqr7VLUEeAAneC0G7neXNUiCLipqB+Li4vTo0aPN2/nXKTDhLjj7VyFLz6Y9R7jg8fmcPagbT14/pv4N3/kpfPEc/HQjRMeH7PzGGBMMESlV1bhwp6OGN3I84OR6QlTHU6NfelfuOieXd1fu5N0VxXVvVF3lzDKae54FHWOMwUuBxxcZkubUtX13Yl+G9kzgP95YxYHSOlrNbV0IR3ZZMZsxxri8E3j8ESHP8YAzRfbDV43gQGk597624uut3Fa9BpFdnByPMcYYDwUeX2TIWrXVNrRnIj85fyDvrtzJC59vO7miugpWz4QB50NUuyleNcaYsPJO4PFHhrRVW223TezLxNw07n9zNet2HnYWFi+D0r0w8KJWO68xxnQ03gk8vohWy/EA+HzCo1ePJD4mkttf+IJj5VWw8UNnZd8prXZeY4zpaLwTeFqhVVtt6fHR/O6aU9iw+wg//9cKdNMcyDgFuqa36nmNMaYj8U7gacU6nkATc9P58TkDmL10A7r9c+h3dquf0xhjOhLvBJ5WatVWlx+d1Z8fZO/Ap1WsiM1rk3MaY0xH4Z3A42v9orYTp/IJ383YTCmxfOcD2F5S2ibnNcaYjsA7gccf1SZFbQCoErl5LuRMpIJIbnrm87o7lxpjjAcFFXgam49bRO525+JeLiJzRKRPwLo65+MWkTEissI95u9FREJzSfVo5ebUX1FSAAe20mXweUy/YQzbS45x67P5HK+oapvzG2NMO9Zo4AlmPm5gKZCnqiOAGcDD7r4Nzcf9JM50qrnuq9HpUluklZtTf8XGOc57/7M5tW8qv7tmJEu27eeul5ZRVd1xBmU1xpjWEEyOp9H5uFV1rqrWVGQswpkMCOqZj1tEMoAEVV2kzhgzzwGXh+B66tcGzalP2DQHkrMhpS8AF43I4D8uGsJ7q3byqzdWBj95nDHGdEIRQWxT15zapzaw/S3Auw3sm+m+CutY3npaaZDQr6ksh83z4ZRpX1l884Qc9hwp48l5m4iN9POLiwbT2qWLxhjTHgUTeIImItcDecDkEB7zNuA2gKioqOYfqK2aU29fBBVHof/X++/87PyBHCuv4qlPNhMT6ecn5w9s/fQYY0w7E0zgCWpObRE5B/gFMFlVywL2nVJr33nu8qxay+ucp9udG3w6OBPBBZHeurVRB1I2znHqk3ImfW2ViHDfJUMoq6zij3M3Eun3cec5ua2fJmOMaUeCqeNpdD5uERkF/AW4VFV3B6yqcz5uVS0GDonIaW5rtm8Db4TgeurXVq3aNs6B3qfXO+mbiPDby4dz5egsfvfBeh5+b63V+RhjPKXRwBPkfNyPAF2Bf4rIMhGZ6e7b0HzcPwCeAjYCmzhZL9Q62qJV2+GdsGsF9Dur4aT4hEeuGsF1p/bmT/M28es3V1Ntrd2MMWESRJeZm0Rkj/t8XyYitwasqwpYPrP2vnUJqo5HVd8B3qm17FcBn89pYN+ngafrWJ4PDAvm/CHRFq3aNrmjUfev989xgs8n/PbyYcRG+vnbJ5spLa/kv64YToTfO316jTHhF9Bl5lychl6LRWSmqq6utenLqnp7HYc4pqojm3LOkDYuaNfaYsicjXMgrht0Dy6eigi/vGgwcdER/H7OBkqOlvOHa0cTG+Vv3XQaY8xJJ7rMAIhITZeZ2oEnZLzz89rfyo0LqqucHE+/s8AX/J9VRLj73AE8cNlQ5qzdzbeeWsT+oza8jjEmpCJEJD/gdVvAuvq6vdR2pTs6zQwRCWxwFuMec5GIBNUf0zuBx9fKzamLl8GxkqCK2epyw+nZ/Om60awsOsSVf17Ilr1HQ5xAY4yHVapqXsBrehP3fxPIdkenmQ08G7Cuj6rmAdcBj4lIv8YO5p3AUzNIaGu1INs4BxDod2azD3HB8Ayev+VU9h8t5/I/LWBRwb7Qpc8YY+rWaJcZVd0X0E3mKWBMwLod7nsBTneZUY2d0EOBJ9J5r26lgTo3zoGeIyEurUWHGZeTwus/HE9qXBQ3/O0zXlm8vfGdjDGm+YLpMpMR8PVSnBbOuF1lot3PacB4gqgb8k7g8bntKFqjnufYfihcHLLZRvukxvHaD8ZzWt9Ufvbqcu57YyUVVdUhObYxxgQKssvMHSKySkS+BO4AbnKXDwby3eVzgQfraA33Nd5p1VaT46mqgMjY0B575augVTD4kpAdMjE2kmduGstD763lr/M3s6b4ME98azTp8dEhO4cxxkBQXWbuBe6tY7+FwPCmns9DOZ6aorZWGL1g6T+cJtQZp4T0sBF+H7+4aAiPTxvJ8h0HuOQPn7B4S0njOxpjTDvmncDjdzN3oW7ZtnsNFH0BI78FrTTa9GUjM3n1+2cQHelj2vRF/PmjTTbSgTGmw/JO4DmR4wlx4Fn2D6f+aMTVoT1uLUN7JvLmjyZw/tDuPPjuWm59Lp99R8oa39EYY9oZ7wSewDqeUKmqgC9fhgFTW9yaLRgJMZE8cd1ofn3pUD7ZsJepj89n3rrdje9ojDHtiHcCT2vU8WycA0d3w8jrQnfMRogIN56RzRu3jye5SyQ3PbOY/5y5iuMVrdRM3BhjQsw7gac16niWPQ9x6ZB7XuiOGaTBGQnMvH0CN52Rzf8t3MLFf/iEFYUH2zwdxhjTVN4JPDU5nqoQjYNWWgLr3oMR15wsxmtjMZF+/vPSoTx38ziOHK/kij8t4PdzNlifH2NMu+adwOMPcVHb+vechgrDrwrN8Vpg0oB0Zt01iYtGZPDo7PVc/sQCVhVZ7scY0z55J/D4QlzUtn4WxGdARpOmoWg1iV0ieXzaKP58/Wh2HSrj0j8u4JFZa63uxxjT7ngn8PijnPdQNKeuLHemQMg9r9X67jTX1GEZfHD3JC4fmckTczdx4ePzbbBRY0y74qHAE8Lm1Ns+hbJDTjPqdiipSxT/e/UpPHfzOCqqq5k2fRH/PmO5zfNjjGkXvBN4Qtmcev0s8EdD38ktP1YrmjQgnffvmsz3JvdjxheFnP3oR7ySv91GPTDGhJV3Ak8om1Ovfw9yJkJUXMuP1cpio/zcc8Eg3r5jAn3T4vjZjOVc/ZdPWV10KNxJM8Z4lHcCT6iGzNm7EUo2tdtitvoM6pHAK//vdB65agQFe49y8R/m8/N/raDEit+MMW3MO4HnRB1PC4va1r/nvIeh02hL+XzCN/N6MfffpnDjGdm8vHg7Ux6Zy1PzCyivtL4/xpi24Z3AE6qJ4DbMgm5DILlPy9MUJoldIrnvkqG8d+dERvZO5jdvr+H8xz7m/c0hZQUAAB2/SURBVFU70daaGtwYY1zeCTyhaNV2/CBsXQgDzg9NmsIst3s8z908jme+Mxa/T7jt70uYNn0Ry7YfCHfSjDGdmHcCTyjqeDZ/7LSK64DFbA05c2A33r1zIg9cNpSNu49w+RML+OELX7Bl79FwJ80Y0wZEZKqIrBORjSJyTx3rbxKRPSKyzH3dGrDuRhHZ4L5uDOZ83pz6urm2LICIWMjMC02a2pFIv48bTs/mitFZTP+4gL9+XMCslTu5dlxv7jg716bcNqaTEhE/8ARwLlAILBaRmaq6utamL6vq7bX2TQHuA/IABZa4++5v6JweyvGEoDn11k+g11iIiApNmtqhrtER3H3uAD762RSmjevFi59vY/Ijc/nf99dxsDTEk+gZY9qDccBGVS1Q1XLgJeCyIPc9H5itqiVusJkNNNrk1zuBx9/CorZj+2HnSugzIXRpase6xcfwm8uHM/vuyZw5qBt/+HAjEx7+kMc+WM+h4xaAjOlEMoHtAd8L3WW1XSkiy0Vkhoj0auK+X+GdwONrYXPqrZ8CCtnjQ5akjiAnLY4nrhvNu3dO5Ix+qTz2wQYmPPghv5+zwQKQMR1HhIjkB7xua+L+bwLZqjoCJ1fzbIsS05KdOxSfH5Dm53i2LnCGyemE9TvBGJyRwF9uyGPljoM8PmcDj85ez98+2cwtE3K4aXw2CTHhmZPIGBOUSlWt7+G1A+gV8D3LXXaCqgaONPwU8HDAvlNq7TuvscR4J8cj4hS3NbeOZ8snkJUHkTGhTVcHMywzkb9+O4+3fjSBsdnJPDp7PeMf/JDfzV5vdUDGdEyLgVwRyRGRKGAaMDNwAxHJCPh6KbDG/TwLOE9EkkUkGTjPXdYg7+R4wClua84goccPws7lMPEnoU9TBzUsM5GnbhzLyh0H+cOHG3h8zgb+9slmvnVab24Zn0O3BG8HaGM6ClWtFJHbcQKGH3haVVeJyP1AvqrOBO4QkUuBSqAEuMndt0REHsAJXgD3q2pJY+eUjtRTPS4uTo8ebUHfkgd7w4hpcOHDjW8baP378MI34dtvQN8pzT9/J7a66BB/mreRd1YUE+H3cdWYLG6b2JfstPY/kKoxnZ2IlKpqu/nP6MEcTzOKg7Z+4uybNS70aeokhvRM4I/XjWbL3qP85eMCZuQX8tLn27hgWAbfm9yP4VmJ4U6iMaad8FbgaW4dz5YFkDkaorqEPk2dTHZaHP/9jeH8+Jxcnlm4hec/3crbK4o5rW8K353YlzMHdsPna1+zthpj2pZ3GhdA8+p4yo5A0VLo461m1C3VLSGGf586iIX3nsUvLhzMtn2l3PJsPuf+7iNe+Gwbxyuqwp1EY0yYBBV4ghjHZ5KIfCEilSJyVa11D4nISvd1TcDys919lonIJyLSv+WX0wh/RNNzPIWfg1Z5rv9OqMTHRPLdSX356Gdn8vi0kcRG+fn5v1acaAm353BZuJNojGljjQaegHF8LgCGANeKyJBam23DaeXwQq19LwJGAyOBU4GfiEiCu/pJ4FuqOtLd75fNv4wg+SKhqokTn+1Y4rxnjQ19ejwk0u/jspGZvHn7BF787mmc0iuJx+dsYPyDH3L3y8v40kbENsYzgqnjOTGOD4CI1Izjc2IAOVXd4q6rPZvYEOBjVa0EKkVkOc44Pq/gDChXE4QSgaLmX0aQ/M0oaitaBqn9IcYqx0NBRDi9Xyqn90tl054jPLdwCzOWFPLa0h2M6p3ETWdkc8GwDKIivFUKbIyXBPO/u1lj8bi+BKaKSBcRSQPO5GQP2VuBd0SkELgBeLCuA4jIbTXDPFRWtnD2UF8zitqKlkHGyJad19SpX3pXfn3ZMBb9/Gzuu2QIB0oruPOlZUx4yCmG23nweLiTaIxpBa36s1JV3wfeARYCLwKfAjW1yj8GLlTVLOAZ4NF6jjFdVfNUNS8iooWN8PxNbE59ZDccKoSeo1p2XtOg+JhIvjM+hzl3T+aZ74xlSM8Efv/hBsY/9CHff34JCzbupbq64/Q3M8Y0LJgneaPj+DREVX8L/BZARF4A1otIOnCKqn7mbvYy8F6wx2w2X2TTBgktWua8W+BpEz6fcObAbpw5sBtb9x3lhc+28Ur+dt5duZOctDiuHdeLq8b0IiWu805LYYwXBJPjaXQcn/qIiF9EUt3PI4ARwPvAfiBRRAa4m57LybF/Wk9TczzFywCBjBGtliRTtz6pcdx74WA+vfdsHrtmJGldo/ivd9Zy2n/P4ccvLyN/SwkdadQNY8xJjeZ4ghnHR0TGAv8CkoFLROTXqjoUiATmiwjAIeB6t6EBIvJd4FW3QcJ+4OZWuL6v8kdC2eHgty9aCmm5EB3femkyDYqJ9HP5qEwuH5XJup2HeeGzrbz2xQ7+tXQHA7p35ZqxvbliVKblgozpQLw1VtsL05w6m+99Etz2/zsIcibBN6Y3/5wm5ErLK5m5rIiXFm9n2fYDRPl9nDu0O9fk9WJ8/zT8NjKCMV9hY7WFkz8i+DqewzvhcLG1aGuHukRFMG1cb6aN683anYd46fPtvL5sB28vLyYzKZYrx2TxzTFZ9EqxIY6MaY+8FXiaMkioNSzoEAb1SOA/Lx3KvRcOYvbqXby8eDt/+HADv5+zgfH9U/nmmF6cP7QHsVH+cCfVGOPyVuBpyiChRUtBfNBjeOumyYREdISfi0f05OIRPdlx4BivLinklfzt3PXyMrpGR3Dh8B5cOTqLsdkpNkipMWHmrcDTlEFCi5dB2gCI7tq6aTIhl5kUyx1n53L7mf35bHMJr35RyNvLi3klv5BeKbFcMSqLK0dn0ie13RR5G+Mp3go8TRkktGgp9DurddNjWpXPd3J4nvsvG8qsVTt5dcmOE0VxY/okc8WoTC4ekUFSF2sVZ0xb8VbgCXaQ0EPFcGSX1e90Il2iIrhiVBZXjMqi+OAxXl9axL+WFvLL11fy6zdXMWVgNy4fmcnZg7sRE2n1Qca0Jm8FnmAHCS1a6rxbi7ZOKSMxlu9P6cf3JvdlVdEhXl+6g5lfFjF79S7ioyM4f1gPLh+Zyen9Uq1ptvEEEZkKPI7TV/MpVa1v7MwrgRnAWFXNF5FsnM7/69xNFqnq9xo7n7cCT7CDhO5cAQj0GNbqSTLhIyIMy0xkWGYi9144mEUF+3h96Q7eW7mTGUsK6RYfzUUjMrj0lJ6M7JWE2xHamE4lYOqbc3EGgV4sIjNVdXWt7eKBO4HPah1ikzu9TdC8FXiCHTJn10pI6QtRVvnsFX6fML5/GuP7p/HA5cP4cO1uXl+6g38s2sYzC7bQO6ULl5ySwcUjejKoR7wFIdOZNDr1jesB4CHgpy09occCTxRoNVRXg6+BYep2rYLuQ9suXaZdiYn0c+HwDC4cnsHBYxXMWrWTN78s4s8fFfDE3E3079aVi0c4Qah/N2v1aDqECBHJD/g+XVVrhmSpa+qbUwN3FpHRQC9VfVtEageeHBFZijMs2i9VdX6jiWly8jsyn3u51RXgi657m/KjUFIAI66pe73xlMTYSK7O68XVeb3Ye6SMd1fu5K0vi3h8zgYe+2ADg3rEc8kpPblweAY5aZZDNu1WparmNWdHEfHhTFtzUx2ri4HeqrpPRMYAr4vIUFU91NAxvRV4/JHOe1UFRNQTeHavBdRyPOZr0rpGc8NpfbjhtD7sOnScd1YU89byYh6ZtY5HZq1jSEYCF43IsCBkOprGpr6JB4YB89wi5h7ATBG5VFXzgTIAVV0iIpuAAUBg7uprvBV4fG7gaaieZ9dK590Cj2lA94QYvjM+h++Mz6HowDHeXbmTt5cXfSUIXTi8BxcMz6BfuhXHmXbtxNQ3OAFnGnBdzUpVPQik1XwXkXnAT9xWbelAiapWiUhfIBcoaOyE3go8J3I8DTSp3rUKorpCUp+2SZPp8HomxXLLhBxumeAEoXdWFPPOimL+5/31/M/76xnYPZ6pw3pw/tAeDM6whgmmfQlm6psGdp8E3C8iFUA18D1VLWnsnN6aFiH/GXjrLrh7DST0rHubZy5yOpneOrv55zEGKD54jFkrd/LOyp0s3lKCKvRO6XIiCI3qlWTjxpk2YdMihFNgHU9dVJ2itqFXtF2aTKeVkRjLTeNzuGl8DnsOl/HBml28t3InzyzYzPSPC+gWH815Q7szdWgGp/ZNIdIfzITAxnR83go8J+p46ilqO1QExw9Y/Y4JufT4aK4d15trx/Xm0PEK5q7dzbsrnLHjnl+0jcTYSM4e1I3zhvZg0oA0ukR567+m8RZv/ev2u5db33htu1Y5791txALTehJiIrlsZCaXjczkWHkV8zfs4b2VO5mzdjevLd1BdISPibnpnDukG2cO6ka3+JhwJ9mYkPJW4PE1UtR2okXbkLZJj/G82Cg/5w3twXlDe1BRVc3izSW8v3oXs1fv4oM1uwAY2SuJ84Z25/yhPayFnOkUvBV4/I00p969GhJ7QUxi26XJGFek38cZ/dM4o38a910yhLU7D/PB6l3MXrOLh99bx8PvraNvehznDO7OWYO6kdcnmQirFzIdkLcCj6+R5tQ2VI5pJ0SEwRkJDM5I4Edn51J04BgfrHFyQv+3YAvTPy4gISaCMwd145zB3Zk8MJ2EmMhwJ9uYoHgr8PgDhsyprbIM9q6HgRe0bZqMCULPpFi+fXo23z49myNllXyyYS8frNnF3LW7eWNZERE+YVxOCmcN6sbZg7vbyAmmXfNY4HFnmayrjmfveqe1m+V4TDvXNTqCqcN6MHVYD6qqlWXb9zN79W4+XLuL37y9ht+8vYactDimDEznrEHdGJeTQnSETW5n2g9vBZ6GmlNbizbTAfl9wpg+KYzpk8I9Fwxie0kpc9bsYu66PfzjM2dKh7goPxNz0zlrcDemDEy3VnIm7LwVeE40p64jx1O8HCJiIKVf26bJmBDqldLlRKfVY+VVLNy0lzlrd/Phmt28t2onAEMyEpg8MJ0zB3ZjdO8ka6Bg2py3Ak9Dg4QWLYUeI04GJ2M6uNgoP2cP7s7Zg7ujlyuriw/x0fo9fLRuD3/9uIAn520iISaCSQPSmTKwG5MGpFluyLQJbz1l6xsyp7oKir+EUde3fZqMaQMiwtCeiQztmcgPpvTn0PEKFmzYy9x1u5m7bg9vLS8GYGjPBCa7gWhU7yQbxse0Cm8FnhMTwdWq49m7HiqOQubotk+TMWGQEBPJBcMzuGB4BtXVX80N/eXjAv40bxPxMRFM6J/G5AHpTB6YTkZibLiTbToJbwWe+nI8O75w3nuOatv0GNMO+HzCsMxEhmUm8sMzT+aG5q3bw0fr9/DuSqduaGD3eCYNSGNibjrjclKIibSWcqZ5vBV46qvjKVrqzMGTmtv2aTKmnQnMDakq63cdYd663czfsJdnF27lr/M3Ex3hY1xOCpMHpDMxN50B3bvaPEMmaN4KPPXleIqWQsZI8Fl5tjGBRISBPeIZ2COe/ze5H8fKq1i0eR/z1+/l4w17+M3ba4A1dE+IZmJuOpMGpDO+XyqpXeuZWt4YvBZ4fHU0p64sh50rYNx3w5MmYzqQ2Cg/Zw7sxpkDuwFQdOAY8zfs4eP1e5m9ehczlhQCTpPtiblpTMhNY2y2FcuZr/JW4KlrkNA9a6CqzBoWGNMMPZNiuWZsb64Z25uqamV54QEWbNzLJxv38vSCzfzl4wKiInyMzU5mQv90JuamMSQjwWZebWdEZCrwOM7U10+p6oP1bHclMAMYq6r57rJ7gVuAKuAOVZ3V2Pm8FXjqGiTUGhYYExJ+nzCqdzKjeidz+1m5lJZX8tnmEj7ZsJdPNuzloffW8tB7kBIXxen9UpnY38kRZSV3CXfSPU1E/MATwLlAIbBYRGaq6upa28UDdwKfBSwbAkwDhgI9gQ9EZICqVjV0Tm8FnrpyPEVLISYJknPCkyZjOqkuURFfKZbbfeg4CzbtZb4biN52+w71Se3CGf3SGN8/ldP7Wv1QGIwDNqpqAYCIvARcBqyutd0DwEPATwOWXQa8pKplwGYR2ege79OGTuitwCPi1PNU1Qo8PUc564wxraZbQgxXjMriilFZqCobdx9h/oa9LNy0l7e+LOLFz7cBMDgjgTP6pTK+fyrjclLpGu2tx1QriRCR/IDv01V1uvs5E9gesK4QODVwZxEZDfRS1bdFJDDwZAKLau2b2WhigklxY+V/IjIJeAwYAUxT1RkB6x4CLnK/PqCqL7vLBfgN8E2cssEnVfX3waSnRXyRJ3M8Fcedyd/OuKPVT2uMOUlEyO0eT273eG6ekENlVTXLdxxk4ca9LNi4j79/upW/fbKZCJ9wSq8kxvdL5Yz+aYzqnWQjbTdPparmNWdHEfEBjwI3hSoxjQaeIMv/trmJ+kmtfS8CRgMjgWhgnoi8q6qH3O17AYNUtVpEurX8coLgjzxZx7NrpTOKgTUsMCasIvw+RvdOZrRbP3S8ooolW/ezYONeFmzaxx/nbuT3H24kJtLH2OwUTuubyun9UhmRmWiDnLbcDpxncY0sd1mNeGAYzvMboAcwU0QuDWLfOgWT42m0/E9Vt7jrqmvtOwT4WFUrgUoRWQ5MBV4Bvg9cp6rV7jF2B5GWlvNFnMzxWMMCY9qlmEg/4/unMb5/GgAHj1XwWcE+Fm7ax6eb9vHIrHWAMzfRuJwUt2gujYHd463FXNMtBnJFJAcnaEwDrqtZqaoHgbSa7yIyD/iJquaLyDHgBRF5FKdxQS7weWMnDCbwNFr+14AvgftE5H+BLsCZnAxY/YBrROQKYA9OM7wNtQ8gIrcBtwFERUUFedoG+CNP1vHsWAJx3SCh0SJJY0wYJcZGct7QHpw3tAcAe4+U8VlBCQs27WXhxr18uNb53ZoSF8WpOSmc3s9pqNC/m42o0BhVrRSR24FZONUpT6vqKhG5H8hX1ZkN7LtKRF7Bea5XAj9srEUbtHLjAlV9X0TGAgtxgsunOPU54BS9HVfVPBH5BvA0MLGOY0wHpgPExcVpixPlizw5SOj2z6DXOGtYYEwHk9Y1motGZHDRiAwAdhw4xqdubmhRwb4T48uldY3mtL4pnNo3ldNyUiwQ1UNV3wHeqbXsV/VsO6XW998Cv23K+YIJPM0qw6srUSLyArDeXVUIvOZ+/hfwTLDHbBG/26rtyB7YvxnyvtMmpzXGtJ7MpFiuGpPFVWOcFnPbS47xacFeJxgV7Dsx7UNqXBSn9U3lNDdH1C89zgJRGAQTeBos/2uI2zAhSVX3icgInFZv77urX8cpetsMTOZkQGpdvkioKodCtxiyV7ClhsaYjkBE6J3ahd6pzogKqsq2klIWFezjs4ISPi3Yx9srnECU1jWaU3NSOLWv02Ah13JEbUJUGy+9EpELcZpL15T//Taw/M8tTvsXkAwcB3aq6lARiQHcGnwOAd9T1WXuMZOAfwC9gSPuui8bSkdcXJwePXq0Odd50p9Oh5S+kNofPn0C7i2ESJt10RivUFW27KsJRPv4bHMJxQePA04d0bhsJxCdmpPKoB6do7GCiJSqaly401EjqMDTXoQk8Px5IsRnQNlhJ+fz3TmhSZwxpkOqKZpbtNnJES0q2MeOA8cAp1HD2OxkxuWkMDY7hWGZiR1yVtb2Fni81yXYHwkVpVD0BeTdEu7UGGPC7GTRXBeuznOqswv3l54IQou3lPDBGqfVXJcoP2P6JHNa31TG5aQwIivROrQ2g/cCjy8SipdD5XGnRZsxxtSSldyFrDFduHJMFuCMM7d4y34+c3NFNf2IoiJ8jMxKcnJEOSmM6ZNsQ/wEwXtFbf93MWyZ73y+ew0k9Gx5wowxnlJytJzFW0pYvLmEz7eUsKroEFXVik9gaM9ExmanuMVzye1i0NP2VtTmvcDz9ytg04eQ2At+vDI0CTPGeNrRskq+2Lb/RCBauu0AZZXOQC790uNO1BGNy0kJyzQQ7S3weC9PWDMnjxWzGWNCJC46gom56UzMTQegrLKKlTsO8vnm/Xy+2elH9OLnzgAwPRNjGJuTQl52Cnl9khnQPR5/J2g51xTeCzw1c/JkWeAxxrSO6Ag/Y/qkMKZPCt+f0o+qamXtzkMs3lzC4i37WbhpH28sKwIgPiaCvD7JjM1JYVx2CsM90GDBe4HH516y5XiMMW3E7xOG9kxkaM9Ebhqfc6IJd/7WEhZvKeHzzSXMXbcHcBosnJKVyJg+To5oTJ9kkuNCME5lO+K9Op5Xb4U1b8G920/mfowxJsz2HSlj8Zb9LNlaQv7W/azccZCKKuf53L9bV5781mhyu8c369hWxxNuo2+E7AkWdIwx7Upq12imDuvB1GHOCNzHK6r4cvsB8rfuJ39LCT0SO88IK97L8RhjjMe0txxPxxv7wRhjTIdmgccYY0ybssBjjDGmTVngMcYY06Ys8BhjjMeJyFQRWSciG0XknjrWf09EVojIMhH5RESGuMuzReSYu3yZiPw5qPNZqzZjjOncGmrV5s4UvR44FyjEmXX6WlVdHbBNgqoecj9fCvxAVaeKSDbwlqoOa0p6LMdjjDHeNg7YqKoFqloOvARcFrhBTdBxxQEtyrFY4DHGmM4vQkTyA163BazLBLYHfC90l32FiPxQRDYBDwN3BKzKEZGlIvKRiEwMKjHNuICwKS0tVRE51szdI4DKUKang/DidXvxmsGb123XHJxYVc1ryUlV9QngCRG5DvglcCNQDPRW1X0iMgZ4XUSG1sohfU2HCjyq2uwcmojkt/QP3xF58bq9eM3gzeu2aw6JHUCvgO9Z7rL6vAQ8CaCqZUCZ+3mJmyMaAOQ3dEIrajPGGG9bDOSKSI6IRAHTgJmBG4hIbsDXi4AN7vJ0t3ECItIXyAUKGjthh8rxGGOMCS1VrRSR24FZgB94WlVXicj9QL6qzgRuF5FzgApgP04xG8Ak4H4RqQCqge+paklj5/RS4Jke7gSEiRev24vXDN68brvmEFDVd4B3ai37VcDnO+vZ71Xg1aaer0P14zHGGNPxWR2PMcaYNmWBxxhjTJvyROBpbByizkBEeonIXBFZLSKrROROd3mKiMwWkQ3ue3K40xpqIuJ3O7C95X7PEZHP3Pv9sttSp1MRkSQRmSEia0VkjYic3tnvtYj82P23vVJEXhSRmM54r0XkaRHZLSIrA5bVeW/F8Xv3+peLyOjwpTx4nT7wuE39ngAuAIYA19YMcNfJVAL/pqpDgNOAH7rXeQ8wR1VzgTnu987mTmBNwPeHgN+pan+cFji3hCVVretx4D1VHQScgnP9nfZei0gmTm/5PHdcMD9Os9/OeK//D5haa1l99/YCnCbMucBtuP1r2rtOH3gIYhyizkBVi1X1C/fzYZwHUSbOtT7rbvYscHl4Utg6RCQLp1/BU+53Ac4CZribdMZrTsRpxvo3AFUtV9UDdPJ7jdMKN1ZEIoAuOL3mO929VtWPgdpNkuu7t5cBz6ljEZAkIhltk9Lm80LgCWocos7EHTF2FPAZ0F1Vi91VO4HuYUpWa3kM+BlOHwKAVOCAqtYMKdIZ73cOsAd4xi1ifEpE4ujE91pVdwD/A2zDCTgHgSV0/ntdo7572yGfb14IPJ4iIl1x2tXfVXu8JHXaznea9vMicjGwW1WXhDstbSwCGA08qaqjgKPUKlbrhPc6GefXfQ7QE2eE5NrFUZ7QGe6tFwJPU8ch6rBEJBIn6PxDVV9zF++qyXq777vDlb5WMB64VES24BShnoVT95HkFsdA57zfhUChqn7mfp+BE4g6870+B9isqntUtQJ4Def+d/Z7XaO+e9shn29eCDyNjkPUGbh1G38D1qjqowGrZnJyeIsbgTfaOm2tRVXvVdUsVc3Gua8fquq3gLnAVe5mneqaAVR1J7BdRAa6i84GVtOJ7zVOEdtpItLF/bdec82d+l4HqO/ezgS+7bZuOw04GFAk1255YuQCEbkQpy6gZhyi34Y5SSEnIhOA+cAKTtZ3/BynnucVoDewFbg6mLGUOhoRmQL8RFUvdgcrfAlIAZYC17uj6HYaIjISp0FFFM6gjN/B+SHZae+1iPwauAanBedS4Fac+oxOda9F5EVgCpAG7ALuA16njnvrBuE/4hQ7lgLfUdUGR4ZuDzwReIwxxrQfXihqM8YY045Y4DHGGNOmLPAYY4xpUxZ4jDHGtCkLPMYYY9qUBR5jjDFtygKPMcaYNvX/ASwxIkXk9UVGAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "ujbwQZlQDqWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 without mini-Batch: in the softmax and the normalization we use \\_shared_ float sum[5000] etc, donc without mini-batch we can't use the entire dataset wich contains N_train = 12000 for training. So we choose the size of N_train = 5000 and size of N_test = 3000, and we plot the loss and accuracy during 1200 iters. After 1200 iters, the final accuracy is 0.791, Wall time: 2.72 s. If we use 2000 iters, the final accuracy is 0.801, Wall time: 4.33 s. \n",
        "<br>\n",
        "<br>\n",
        "Step 2 with mini-Batch: we use N_train = 12000 for training and  N_test = 5000 for testing. We plot the loss and accuracy during 100 epochs. after 100 epochs, the final accuracy is 0.738, Wall time: 2.72 s. If we use 200 epochs, the final accuracy is 0.745, Wall time: 1.72 s. \n",
        "<br>parameters: learning rate = 0.005, batch size = 100\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9kxdbjgB8H0g"
      }
    }
  ]
}